# Dimensionality reduction and clustering

## Visually extracting information from data

:::: {.columns}

::: {.column width="50%"}
### Data

<br/>

::: {.small-table}
```{r}
knitr::kable(head(assay(airway, "counts"), 20), format = "html", escape = FALSE)
```
:::
:::

::: {.column width="50%"}
### Information

```{r}
#| fig-align: center
#| fig-height: 8
#| fig-width: 7
top_var_idx <- head(order(rowVars(assay(airway, "logcounts")), decreasing = TRUE), 40)
Heatmap(
  matrix = assay(airway, "logcounts")[top_var_idx, ],
  row_names_gp = gpar(fontsize = 8),
  name = "logcounts"
)
```
:::

::::

## Sources of variation in data

Variation in data (e.g., expression) can come from multiple sources.

:::: {.columns}

::: {.column width="50%"}
### Biological

- Stimulus (e.g., infection, drug)
- Condition (e.g., healthy, disease)
- ...
:::

::: {.column width="50%"}
### Technical

- Instrumentation
- Operator
- ...
:::

::::

Whether each source of variation is seen as signal or noise depends on the goal of the study.

## Confounding variables

Experimental design is crucial to ensure that sources of interesting variation are not confounded with independent sources of uninteresting variation.

:::: {.columns}

::: {.column width="50%"}
### Confounded

<br/>

```{r}
nrow <- 8
data.frame(
  Cell = seq_len(nrow),
  Site = rep(c("S1", "S2"), each = 4),
  Treatment = rep(c("A", "B"), each = 4)
) %>%
  mutate(
    Site = cell_spec(Site, "html", color = ifelse(Site == "S2", "red", "blue")),
    Treatment = cell_spec(Treatment, "html", color = ifelse(Treatment == "A", "cyan", "orange"))
  ) %>% 
  knitr::kable(format = "html", escape = FALSE)
```
:::

::: {.column width="50%"}
### Balanced

<br/>

```{r}
nrow <- 8
data.frame(
  Cell = seq_len(nrow),
  Site = rep(c("S1", "S2"), each = 4),
  Treatment = rep(c("A", "B"), times = 4)
) %>%
  mutate(
    Site = cell_spec(Site, "html", color = ifelse(Site == "S2", "red", "blue")),
    Treatment = cell_spec(Treatment, "html", color = ifelse(Treatment == "A", "cyan", "orange"))
  ) %>% 
  knitr::kable(format = "html", escape = F)
```

:::

::::

## Feature selection

Many genes are not interesting because they don't vary much, or they
donâ€™t have enough counts.

Filtering for feature selection is needed to:

- Select genes that display useful variation.
- Reduce memory usage and computational cost/time.

```{r}
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
set.seed(1)
mat <- matrix(c(rnorm(12*4, 0, 10), rnorm(12*4, 0, 1)), nrow = 8, ncol = 12, byrow = TRUE)
ComplexHeatmap::Heatmap(mat, name = "values")
```

## Dimensionality reduction {.smaller}

We use dimensionality reduction methods to:

- Find structure in the data.

- Aid in visualization.

Unsupervised learning helps finding groups of homogeneous items

- Many approaches to do this (e.g. PCA, t-SNE, UMAP)

:::: {.columns}

::: {.column width="70%"}
#### High dimensional

```{r}
nrow <- 10
ncol <- 5
rownames <- paste("gene", seq_len(nrow))
colnames <- paste("sample", seq_len(ncol))
x <- matrix(data = rbinom(nrow*ncol, 10E3, 1E-4), nrow = nrow, ncol = ncol, dimnames = list(rownames, colnames))
knitr::kable(x, format = "html", escape = T)
```
:::

::: {.column width="30%"}
#### Reduced dimensionality

```{r}
nrow <- 5
ncol <- 2
rownames <- paste("sample", seq_len(nrow))
colnames <- paste("dim", seq_len(ncol))
x <- matrix(data = rnorm(nrow*ncol, 0, 10), nrow = nrow, ncol = ncol, dimnames = list(rownames, colnames))
knitr::kable(x, format = "html", escape = T)
```
:::

::::

## Principal component analysis (PCA)

:::: {.columns}

::: {.column width="50%"}
### Goals

- Find linear combination of variables to create principal components (PCs).
- Maintain most of the variance in the data (for given number of PCs).
- PCs are uncorrelated (orthogonal to each other) and ordered with respect to the percentage of variance explained.

### Assumptions
- Relationship between variables is linear!
- Not optimal for non-linear data structures.
:::

::: {.column width="50%"}
```{r}
#| fig-align: center
#| fig-height: 7
#| fig-width: 9
data_x <- tibble(
  x = rnorm(n = 100, mean = 0, sd = 1)
) %>% 
  mutate(
  y = x + rnorm(n = 100, mean = 0, sd = 0.8)
  )
data_pca <- prcomp(data_x)
data_eigengenes <- as_tibble(t(data_pca$rotation * rep(data_pca$sdev^2, each=2))) %>% 
  mutate(
    xend = 0,
    yend = 0,
    PC = paste0("PC", 1:2))
ggplot() +
  geom_point(aes(x, y), data_x) +
  geom_segment(
    aes(x = xend, y = yend, xend=x, yend=y, color = PC), data_eigengenes,
    size=1.25,
    arrow = arrow(length = unit(10, "points"), angle = 30)) +
  labs(x = "gene 1", y = "gene 2") +
  theme_bw() +
  theme(text = element_text(size = 20))
```
:::

::::

```{r}
#| echo: true
#| eval: false
pca <- prcomp(x, center = TRUE, scale. = FALSE, ...)
```

Each principal component is described as a linear combination of the original dimensions.

$$PC_{i} = \beta_{(i,1)} * gene_1 + \beta_{(i,2)} * gene_2$$

## Decomposition into principal components

:::: {.columns}

::: {.column width="50%"}
```{r}
#| fig-align: center
knitr::include_graphics("img/pca.png")
```
:::

::: {.column width="50%"}
### Terminology

$A$ is the original data matrix.

$V$ is calls the **loadings** matrix.

$U$ is the **scores** matrix.
:::

::::

### Interpretation

**Loadings** can be understood as the weights for each original variable when calculating the principal component.
$V$ is also often called the **rotation** matrix.

**Scores** contain the original data in a rotated coordinate system.
$U$ is the matrix of new coordinates used to produce PCA plots.

::: {.notes}
It can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix.
[Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)

Remember that eigendecomposition works on a square matrix (https://notes.andrewgurung.com/data-science/linear-algebra/eigenvalues-and-eigenvectors).
The covariance matrix is square, not the original data matrix.
:::

## General advice for PCA

::: {style="text-align: center;"}
Always **center** data (the default in `prcomp()`).
:::

- This is key to computing the *covariance* matrix.

::: {style="text-align: center;"}
If comparing different units, **scale** data.
:::

- Uses the *correlation* matrix rather than the *covariance* matrix.
- Not necessary for log-normalised gene expression.

<https://towardsdatascience.com/let-us-understand-the-correlation-matrix-and-covariance-matrix-d42e6b643c22>

::: {style="text-align: center;"}
Use a **subset** of the principal components in downstream analyses.
:::

- PCA *rotates* the data, it does not reduce dimensionality.
- Principal components are ordered by decreasing variance captured.
- Extract the first $k$ components to reduce dimensionality
  while preserving as much variance as desired.

::: {.notes}
Centering data for PCA:
<https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca>

DESeq2 PCA:
<https://github.com/thelovelab/DESeq2/blob/e67c68886bf07b90a12594e16d533b42340ad63b/R/plots.R#L249>
:::