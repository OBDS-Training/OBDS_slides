# Statistics

## <i class="fab fa-r-project"></i> is built for statistics

- <i class="fab fa-r-project"></i> includes a number of common statistical distributions:
  - The *Normal* Distribution
  - The *Binomial* Distribution
  - The *Poisson* Distribution
  - ...
- <i class="fab fa-r-project"></i> implements a range of statistical tests:
  - Student's t-Test
  - Pearson's Chi-squared Test for Count Data
  - Wilcoxon Rank Sum and Signed Rank Tests
  - ...

## <i class="fab fa-r-project"></i> Functions for Probability Distributions {.smaller}

:::: {.columns}

::: {.column width="60%"}
::: {.small-table}
|Distribution                   |Probability |Quantile    |Density     |Random      |
|:------------------------------|:-----------|:-----------|:-----------|:-----------|
|Beta                           |`pbeta`     |`qbeta`     |`dbeta`     |`rbeta`     |
|Binomial                       |`pbinom`    |`qbinom`    |`dbinom`    |`rbinom`    |
|Cauchy                         |`pcauchy`   |`qcauchy`   |`dcauchy`   |`rcauchy`   |
|Chi-Square                     |`pchisq`    |`qchisq`    |`dchisq`    |`rchisq`    |
|Exponential                    |`pexp`      |`qexp`      |`dexp`      |`rexp`      |
|F                              |`pf`        |`qf`        |`df`        |`rf`        |
|Gamma                          |`pgamma`    |`qgamma`    |`dgamma`    |`rgamma`    |
|Geometric                      |`pgeom`     |`qgeom`     |`dgeom`     |`rgeom`     |
|Hypergeometric                 |`phyper`    |`qhyper`    |`dhyper`    |`rhyper`    |
|Logistic                       |`plogis`    |`qlogis`    |`dlogis`    |`rlogis`    |
|Log Normal                     |`plnorm`    |`qlnorm`    |`dlnorm`    |`rlnorm`    |
|Negative Binomial              |`pnbinom`   |`qnbinom`   |`dnbinom`   |`rnbinom`   |
|Normal                         |`pnorm`     |`qnorm`     |`dnorm`     |`rnorm`     |
|Poisson                        |`ppois`     |`qpois`     |`dpois`     |`rpois`     |
|Student t                      |`pt`        |`qt`        |`dt`        |`rt`        |
|Studentized Range              |`ptukey`    |`qtukey`    |`dtukey`    |`rtukey`    |
|Uniform                        |`punif`     |`qunif`     |`dunif`     |`runif`     |
|Weibull                        |`pweibull`  |`qweibull`  |`dweibull`  |`rweibull`  |
|Wilcoxon Rank Sum Statistic    |`pwilcox`   |`qwilcox`   |`dwilcox`   |`rwilcox`   |
|Wilcoxon Signed Rank Statistic |`psignrank` |`qsignrank` |`dsignrank` |`rsignrank` |
:::
:::

::: {.column width="40%"}
- Each distribution has a root name, e.g. `norm`
- Every distribution has four functions.
- The root name is prefixed by one of the letters:
  - `p` for "probability", the cumulative distribution function (c. d. f.)
  - `q` for "quantile", the inverse c. d. f.
  - `d` for "density", the density function (p. f. or p. d. f.)
  - `r` for "random", a random variable having the specified distribution

:::

::::

::: {.notes}
Source: <https://www.stat.umn.edu/geyer/old/5101/rlook.html>
:::

## The normal distribution

### Notation

$${\mathcal{N}}(\mu ,\sigma^{2})$$

<br/>

### Parameters

- ${\mu \in \mathbb {R} }$ = mean (location)
- ${ \sigma ^{2}>0}$ = variance (squared scale)

<br/>

### Properties

:::: {.columns}

::: {.column width="50%"}
- Median: ${ \mu }$

- Mode: ${ \mu }$
:::

::: {.column width="50%"}
- Variance: ${ \sigma ^{2} }$
:::

::::

- Probability density function (PDF): ${\displaystyle {\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}}$

## The standard normal distribution

```{r}
slide_mean <- 0; slide_sd <- 1;
```

Standard normal distribution with mean `r slide_mean` and standard deviation `r slide_sd`.

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  pnorm = pnorm(q = quantile, mean = slide_mean, sd = slide_sd)
)
gg1 <- ggplot(x) + geom_point(aes(quantile, pnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("pnorm(q = quantiles, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the cumulative distribution function (c. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = 1, by = 0.01),
  qnorm = qnorm(p = quantile, mean = slide_mean, sd = slide_sd)
)
gg2 <- ggplot(x) + geom_point(aes(quantile, qnorm)) +
  geom_hline(yintercept = slide_mean, color = "blue") +
  geom_label(aes(y = y, label = y), x = 0, data = tibble(y = slide_mean)) +
  geom_hline(yintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(y = y, label = label), x = 0, alpha = 0.5,
    data = tibble(y = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("qnorm(p = probabilities, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the inverse c. d. f.") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  dnorm = dnorm(x = quantile, mean = slide_mean, sd = slide_sd)
)
gg3 <- ggplot(x) + geom_point(aes(quantile, dnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("dnorm(x = quantiles, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the density function (p. f. or p. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  rnorm = rnorm(n = 1E3, mean = slide_mean, sd = slide_sd)
)
gg4 <- ggplot(x, aes(rnorm)) + geom_histogram(bins = 30, color = "black", fill = "grey") +
  geom_rug(alpha = 0.5) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("rnorm(n = 1E3, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "a random variable having the specified distribution") +
  cowplot::theme_cowplot()
```

```{r}
#| fig-align: center
#| fig-width: 12
#| fig-height: 7
plot_grid(gg1, gg2, gg3, gg4, ncol = 2, nrow = 2)
```

## A parameterised normal distribution

```{r}
slide_mean <- 50; slide_sd <- 100;
```

Normal distribution parameterised with mean `r slide_mean` and standard deviation `r slide_sd`.

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  pnorm = pnorm(q = quantile, mean = slide_mean, sd = slide_sd)
)
gg1 <- ggplot(x) + geom_point(aes(quantile, pnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("pnorm(q = quantiles, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the cumulative distribution function (c. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = 1, by = 0.01),
  qnorm = qnorm(p = quantile, mean = slide_mean, sd = slide_sd)
)
gg2 <- ggplot(x) + geom_point(aes(quantile, qnorm)) +
  geom_hline(yintercept = slide_mean, color = "blue") +
  geom_label(aes(y = y, label = y), x = 0, data = tibble(y = slide_mean)) +
  geom_hline(yintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(y = y, label = label), x = 0, alpha = 0.5,
    data = tibble(y = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("qnorm(p = probabilities, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the inverse c. d. f.") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  dnorm = dnorm(x = quantile, mean = slide_mean, sd = slide_sd)
)
gg3 <- ggplot(x) + geom_point(aes(quantile, dnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("dnorm(x = quantiles, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the density function (p. f. or p. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  rnorm = rnorm(n = 1E3, mean = slide_mean, sd = slide_sd)
)
gg4 <- ggplot(x, aes(rnorm)) + geom_histogram(bins = 30, color = "black", fill = "grey") +
  geom_rug(alpha = 0.5) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("rnorm(n = 1E3, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "a random variable having the specified distribution") +
  cowplot::theme_cowplot()
```

```{r}
#| fig-align: center
#| fig-width: 12
#| fig-height: 7
plot_grid(gg1, gg2, gg3, gg4, ncol = 2, nrow = 2)
```

## A parameterised binomial distribution

```{r}
slide_size <- 50; slide_prob <- 0.1;
```

Binomial distribution parameterised with size `r slide_size` and probability `r slide_prob`.
This distribution models an experiment where a coin is tossed 50 times, and the probability of observing head is 10%.

```{r}
x <- tibble(
  quantile = seq(from = 0, to = slide_size, by = 1),
  pbinom = pbinom(q = quantile, size = slide_size, prob = slide_prob)
)
gg1 <- ggplot(x) + geom_point(aes(quantile, pbinom)) +
  labs(
    title = sprintf("pbinom(q = quantiles, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "the cumulative distribution function (c. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = 1, by = 0.01),
  qbinom = qbinom(p = quantile, size = slide_size, prob = slide_prob)
)
gg2 <- ggplot(x) + geom_point(aes(quantile, qbinom)) +
  labs(
    title = sprintf("qbinom(p = probabilities, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "the inverse c. d. f.") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = slide_size, by = 1),
  dbinom = dbinom(x = quantile, size = slide_size, prob = slide_prob)
)
gg3 <- ggplot(x) + geom_point(aes(quantile, dbinom)) +
  labs(
    title = sprintf("dbinom(x = quantiles, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "the density function (p. f. or p. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  rbinom = rbinom(n = 1E3, size = slide_size, prob = slide_prob)
)
gg4 <- ggplot(x, aes(rbinom)) + geom_histogram(bins = 30, color = "black", fill = "grey") +
  labs(
    title = sprintf("rbinom(n = 1E3, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "a random variable having the specified distribution") +
  cowplot::theme_cowplot()
```

```{r}
#| fig-align: center
#| fig-width: 12
#| fig-height: 7
plot_grid(gg1, gg2, gg3, gg4, ncol = 2, nrow = 2)
```

## The binomial distribution

```{r}
#| fig-align: center
## Source: Kevin Rue-Albrecht (Adobe Illustrator)
knitr::include_graphics("img/binomial-coins.svg")
```

#### Two mutually exclusive outcomes

::: {style="text-align: center;"}
$$P(H) = 1 - P(T)$$
:::

#### Size of experiment

::: {style="text-align: center;"}
Number of times the coin is tossed.
:::

#### Model

::: {style="text-align: center;"}
Probability of observing $H$ head when tossing the coin $N$ times.
:::

## Quantiles

For instance, the minimum value, the value that separates the lowest 10% values in the distribution, 20%, and so on, until the maximum value.

```{r}
#| fig-align: center
#| fig-height: 6
#| fig-width: 9
slide_mean <- 50; slide_sd <- 100;
slide_quantiles_code <- "seq(0, 1, by = 0.1)"
slide_quantiles <- eval(parse(text = slide_quantiles_code))
x1 <- tibble(
  rnorm = rnorm(n = 10E3, mean = slide_mean, sd = slide_sd)
)
x2 <- tibble(
  quantile = slide_quantiles,
  value = quantile(x1$rnorm, slide_quantiles)
)
ggplot() +
  geom_histogram(aes(rnorm), x1, bins = 30, color = "black", fill = "grey") +
  geom_vline(aes(xintercept = value), x2, color = "blue", linetype = "dashed") +
  labs(
    title = sprintf("quantile(x, %s)", slide_quantiles_code),
    subtitle = sprintf("Ten deciles computed on the distribution rnorm(n = 10E3, mean = %s, sd = %s)", slide_mean, slide_sd)) +
  cowplot::theme_cowplot()
```

## <i class="fab fa-r-project"></i> Functions for Statistical Testing

In the `r BiocStyle::CRANpkg("stats")` package, functions that implement statistical tests have a named that ends in `.test`.

```{r}
#| results: asis
cat(paste(sprintf("*%s()*", sort(grep("\\.test$", ls("package:stats"), value=TRUE))), collapse = " -- "))
```

<br/>

Each of those functions comes with a help page with programmatic usage, statistical advice, and external references to published work.

```{r}
#| echo: true
#| eval: false
?pairwise.t.test
help(pairwise.t.test)
```

## Parametric tests and Non-parametric equivalents

When parametric assumptions are not met, non-parametric tests equivalent should be used.

```{r}
#| include: true
#| results: asis
tibble(
  "Parametric test" = c(
    "Paired t-test",
    "Unpaired t-test",
    "Pearson correlation",
    "One-way Analysis of Variance"),
  "Non-parametric equivalent" = c(
    "Wilcoxon Rank sum test",
    "Mann-Whitney U test",
    "Spearman correlation",
    "Kruskal–Wallis test")
) %>% knitr::kable(align = "c")
```

Non-parametric tests make fewer assumptions, as such:

- they have wider applicability.
- they may be applied in situations where less is known about the data.
- they are more robust.
- ..., however, fewer assumption gives non-parametric tests _less_ power than their parametric equivalent.

<https://www.healthknowledge.org.uk/public-health-textbook/research-methods/1b-statistical-methods/parametric-nonparametric-tests>

## Parametric t-test {.smaller}

:::: {.columns}

::: {.column width="50%"}
A $t$-test should only be applied to compare two normal distributions.

```{r}
#| echo: true
set.seed(1)
x <- rnorm(n = 1000, mean = 0, sd = 1)
y <- rnorm(n = 2000, mean = 1, sd = 1)
```

```{r}
#| fig-align: center
#| fig-height: 6
#| fig-width: 7
test_data <- bind_rows(
  tibble(group = "x", value = x),
  tibble(group = "y", value = y)
)
ggplot(test_data, aes(value)) +
  geom_histogram(fill = "grey", color = "black") +
  facet_wrap(~group, ncol = 1) +
  cowplot::theme_cowplot()
```
:::

::: {.column width="50%"}
An unpaired t-test should be used when there is no pairing relationship between observations in the two groups.

```{r}
#| echo: true
t.test(x, y)
```

The second argument is used as reference (i.e., denominator).

```{r}
#| echo: true
t.test(y, x)
```
:::

::::

## Paired test {.smaller}

For each sample, the two measurements are related to one another; e.g. patients measured before and after a treatment.

:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: true
set.seed(1)
n_sample <- 10
x <- runif(n = n_sample, min = 10, max = 20)
y <- x + 2 + rnorm(n = n_sample, mean = 0, sd = 1)
```

```{r}
test_data <- tibble(
  sample = paste("sample", seq_len(n_sample)),
  x = x,
  y = y
) %>% pivot_longer(cols = c(x, y), names_to = "variable")
```

```{r}
#| fig-align: center
#| fig-height: 6
#| fig-width: 7
ggplot(test_data, aes(variable, value)) +
  geom_line(aes(group = sample), size = 0.1) +
  geom_point() +
  labs(
    title = "Paired <x> and <y> observations"
  ) +
  cowplot::theme_cowplot() +
  theme(
    plot.title = element_text(hjust = 0.5)
  )
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
t.test(x, y, paired = TRUE)
```

```{r}
#| fig-align: center
#| fig-height: 5
#| fig-width: 7
test_data %>% 
  pivot_wider(names_from = "variable", values_from = "value") %>% 
  mutate(x_minus_y = x - y) %>%
  ggplot(aes(x_minus_y)) +
  geom_histogram(color = "black", fill = "grey") +
  geom_density(linetype = "dashed") +
  geom_vline(xintercept = 0, color = "red") +
  labs(
    title = "<x> minus paired <y>"
  ) +
  cowplot::theme_cowplot() +
  theme(
    plot.title = element_text(hjust = 0.5)
  )
```
:::

::::

**Note:**
What is actually tested is whether the mean of the differences between the paired $x$ and $y$ measurements is different from 0.

## Non-parametric wilcoxon test {.smaller}

:::: {.columns}

::: {.column width="50%"}
A non-parametric test do not require samples to follow any particular distribution.

```{r}
#| echo: true
set.seed(1)
x <- runif(n = 20, min = 1, max = 11)
y <- runif(n = 10, min = 3, max = 13)
```

```{r}
#| fig-align: center
#| fig-height: 5
#| fig-width: 7
test_data <- bind_rows(
  tibble(group = "x", value = x),
  tibble(group = "y", value = y)
)
gg <- ggplot(test_data, aes(value)) +
  facet_wrap(~group, ncol = 1) +
  geom_histogram(fill = "grey", color = "black") +
  cowplot::theme_cowplot()
gg
```
:::

::: {.column width="50%"}
As per the `?wilcox.test` help page:

> Performs one- and two-sample Wilcoxon tests on vectors of data; the latter is also known as ‘Mann-Whitney’ test.

This is a two-group comparison, thus a 'Mann-Whitney' test.

```{r}
#| echo: true
wilcox.test(x, y)
```

<br/>

Many statistical tests offer the possibility to perform a 'one-tailed' test,
also known as specifying a *directed* hypothesis.

This will often produce a smaller $P$-value when the data match the direction of the hypothesis,
at the expense of being blind to the possibility of the data moving in the direction *opposite* to the hypothesis.

```{r}
#| echo: true
wilcox.test(x, y, alternative = "less")
```
:::

::::

## Analysis of Variance (ANOVA) {.smaller}

:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: true
set.seed(1)
n_sample <- 1000
x1 <- rnorm(n = n_sample, mean = 10, sd = 2)
x2 <- x1 + 5 + rnorm(n = n_sample, mean = 0, sd = 1)
x3 <- x2 + 0 + rnorm(n = n_sample, mean = 0, sd = 0.5)
test_data <- data.frame(
  value = c(x1, x2, x3),
  group = c(
    rep("x1", length(x1)),
    rep("x2", length(x2)),
    rep("x3", length(x3))
  )
)
```

```{r}
#| fig-align: center
#| fig-height: 6
#| fig-width: 7
gg <- ggplot(test_data, aes(value)) +
  facet_wrap(~group, ncol = 1) +
  geom_histogram(fill = "grey", color = "black") +
  cowplot::theme_cowplot()
gg
```
:::

::: {.column width="50%"}
ANOVA can compare any number of groups.
As such, the `aov()` function takes:

- the data as a *data.frame* object
- the name of the column that contains the value
- the name of the column that contains the grouping information

The latter two pieces of information are supplied as a *formula*.

```{r}
#| echo: true
out <- aov(value ~ group, test_data)
out
```

The output of `aov()` is an object that can be statistically summarised using the function `summarise()`.

```{r}
#| echo: true
summary(out)
```
:::

::::

## Fisher's Exact Test

Test of independence between two categorical variables.

Alternative to the Chi-square test when the sample is not "large enough".

- Rule of thumb: when any of the _expected_ values in the contingency table is less than 5.
- Common situation for gene-set over-representation analysis (ORA).

:::: {.columns}

::: {.column width="50%"}
```{r}
#| fig-align: center
## Source: Kevin Rue-Albrecht (Adobe Illustrator)
knitr::include_graphics("img/fisher-test-table.svg")
```
:::

::: {.column width="50%"}
```{r}
#| fig-align: center
## Source: Kevin Rue-Albrecht (Adobe Illustrator)
knitr::include_graphics("img/fisher-test-venn.svg")
```
:::

::::

<https://towardsdatascience.com/fishers-exact-test-in-r-independence-test-for-a-small-sample-56965db48e87>

## Fisher’s Exact Test in action

```{r}
#| echo: true
data_xtable <- matrix(
  data = c(12, 4, 3, 23),
  nrow = 2, ncol = 2,
  dimnames = list(
    c("DE", "Not DE"),
    c("In pathway", "Not in pathway")
  )
)
```

```{r}
data_xtable %>% 
   knitr::kable(align = "c")
```

```{r}
#| echo: true
fisher.test(data_xtable)
```

## Beware of interpreting inadequate tests!

::: {style="text-align: center;"}
e.g.
:::

<br/>

::: {style="text-align: center;"}
You *can* run a $t$-test on data that is *not* normally distributed.
:::

<br/>

::: {style="text-align: center;"}
<i class="fab fa-r-project"></i> *won't* stop you or even warn you.
:::

<br/>

::: {style="text-align: center;"}
But you *shouldn't*.
:::

<br/>

::: {style="text-align: center;"}
And you **definitely** *shouldn't* trust the result of that test.
:::

## Choosing a test {.smaller}

:::: {.columns}

::: {.column width="70%"}
```{r}
#| fig-align: center
#| out-height: 500px
#| out-width: 700px
# Source: https://python.plainenglish.io/statistical-tests-with-python-880251e9b572
knitr::include_graphics("img/choose-statistical-test.png")
```
:::

::: {.column width="30%"}
#### See also

"Operating with Data - Statistics for the Cardiovascular Surgeon: Part III. Comparing Groups"
`r Citep(bib, "liguori2018data")`

"Choosing the correct statistical test in SAS, Stata, SPSS and R"
<https://stats.idre.ucla.edu/other/mult-pkg/whatstat/>
:::

::::

## References

```{r}
#| results: asis
PrintBibliography(bib)
```
