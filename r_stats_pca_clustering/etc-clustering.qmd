## Cluster analysis

Task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups.

:::: {.columns}

::: {.column width="50%"}
```{r}
#| fig-align: center
#| fig-height: 6
ggplot(iris, aes(Sepal.Length, Sepal.Width)) +
  geom_point(size = 3) +
  cowplot::theme_cowplot() +
  labs(x = "variable 1", y = "variable 2")
```
:::

::: {.column width="50%"}
```{r}
#| fig-align: center
#| fig-height: 6
iris %>% 
  mutate(
    Cluster = Species %>% 
      fct_recode(
        "1" = "setosa",
        "2" = "versicolor",
        "3" = "virginica"
      )
  ) %>% 
  ggplot(aes(Sepal.Length, Sepal.Width, color = Cluster)) +
  geom_point(size = 3) +
  cowplot::theme_cowplot() +
  labs(x = "variable 1", y = "variable 2")
```
:::

::::

- Unsupervised machine learning task.
- Many methods (e.g.,$k$-means clustering, hierarchical clustering).
- Insights arise from downstream analyses of the resulting clusters.
- Often an iterative process as a result of initial over- or under-clustering.

## $k$-means clustering

:::: {.columns}

::: {.column width="33%"}

<br/>

1. Initialise $k$ centroids randomly.
2. Assign each data points to the nearest centroid.
3. Compute new centroid coordinates.
4. Repeat (2) and (3) until convergence, or for a maximum number of iterations allowed.
:::

::: {.column width="66%"}
```{r}
#| fig-align: center
#| out-height: 400px
#| out-width: 600px
## Source: https://stanford.edu/~cpiech/cs221/handouts/kmeans.html
knitr::include_graphics("img/kmeans-steps.png")
```

:::

::::

:::: {.columns}

::: {.column width="50%"}
#### Pro(s)

- Easy to understand and implement.
- Extremely fast.
:::

::: {.column width="50%"}
#### Con(s)

- Must pre-select the number of groups.
- Stochastic (must set seed for reproducibility).
:::

::::

## K-Means Clustering - How many clusters?

```{r}
#| fig-align: center
#| fig-height: 7
#| fig-width: 9
set.seed(1)
airway_logcounts <- assay(airway, "logcounts")[pca_use_rows, ]
airway_logcounts %>%
  t() %>% 
  as_tibble() %>% 
  mutate(
    "k = 1" = as.factor(NA),
    "k = 2" = as.factor(kmeans(x = t(airway_logcounts), centers = 2)$cluster),
    "k = 4" = as.factor(kmeans(x = t(airway_logcounts), centers = 4)$cluster),
    "k = 6" = as.factor(kmeans(x = t(airway_logcounts), centers = 6)$cluster)
  ) %>% 
  dplyr::select(starts_with("k = ")) %>% 
  bind_cols(
    airway_pca$x[keep_samples, c("PC1", "PC2")] %>% as_tibble()) %>% 
  pivot_longer(cols = starts_with("k ="), names_to = "k", values_to = "cluster") %>% 
  ggplot(aes(PC1, PC2, color = cluster)) +
  geom_point(size = 3) +
  facet_wrap(~k) +
  theme_cowplot() +
  theme(
      panel.border = element_rect(color = "black")
  )
```

## K-means clustering

To choose $k$, run multiple values identify the one that
maximises the distance between data points of different clusters,
while minimising the distance between data points within each clusters.

- **Sum of squares between clusters:** how far points are _between_ clusters (separation).
- **Sum of squares within clusters:** how close points are _within_ clusters (compactness).
- **Total sum of squares:** overall scatter in the data set.

For good clustering we want small `sum(withinss)` and large `betweenss`.

```{r}
kmeans_scree <- lapply(seq_len(nrow(airway_pca$x)-1), function(x) {
  out <- kmeans(x = t(airway_logcounts), centers = x)
  tibble(
    k = x,
    totss = out$totss,
    betweenss = out$betweenss,
    withinss_sum = sum(out$withinss)
  )
}) %>% 
  bind_rows()
```

:::: {.columns}

::: {.column width="50%"}
```{r}
#| fig-align: center
#| fig-height: 5
#| fig-width: 7
kmeans_scree %>% 
  ggplot(aes(k, betweenss / totss)) +
  geom_line() +
  geom_point() +
  labs(title = "betweenss / totss") +
  scale_y_continuous(limits = c(0, 1)) +
  theme_cowplot()
```
:::

::: {.column width="50%"}
```{r}
#| fig-align: center
#| fig-height: 5
#| fig-width: 7
kmeans_scree %>% 
  ggplot(aes(k, withinss_sum)) +
  geom_line(linetype = "F1") +
  geom_point() +
  labs(title = "sum(withinss)") +
  theme_cowplot()
```
:::

::::

## Hierarchical clustering

$$Distances \rightarrow Tree \rightarrow Clusters.$$

:::: {.columns}

::: {.column width="50%"}
```{r}
#| fig-align: center
#| out-height: 250px
#| out-width: 300px
## Source: https://en.wikipedia.org/wiki/Hierarchical_clustering
knitr::include_graphics("img/hierarchical-clustering-scatter.png")
```
:::

::: {.column width="50%"}
```{r}
#| fig-align: center
#| out-height: 250px
#| out-width: 300px
## Source: https://en.wikipedia.org/wiki/Hierarchical_clustering
knitr::include_graphics("img/hierarchical-clustering-dendrogram.png")
```
:::

::::

### Key parameters

- Distance metric
- Agglomeration method
- Number of cluster / Height of cut
