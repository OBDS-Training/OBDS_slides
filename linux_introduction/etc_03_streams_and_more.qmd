# Talk 3: Working with files and streams in Linux

## Overview

- Linux streams, pipes & redirection
- File compression
- Filter and sort file contents
- Searching for files
- Loops in Bash

## Linux streams to pass information to and from files 

- Streams are mechanisms to move data from one place to another 
<!-- - Few common scenarios that use streams: -->
<!--   + Passing the contents – not the name! – of a file as the input to a command -->
<!--   + Passing the output of a command as the input to another command -->
<!--   + Writing the output of a command to a (new or existing) file -->

- Standard streams:
  + **Standard input (stdin)**: the default place from which input to the system is taken
  + **Standard output (stdout)**: the default place where a process (i.e. command) can write output
  + **Standard error (stderr)**: the default place where a process (i.e. command) can write error messages

- By default stdout and stderr both print to the terminal but their outputs can be redirected to other destinations (most commmonly, files)
- Streams can be redirected to/from files

## Redirecting streams (input, output and error)

- Streams can be redirected to new destinations – including files – using the symbols '>', '<', and variants thereof

```
$ command1 < file1      # Input file1 to stdin for command1
$ command1 > file1      # Write standard output of command1 to file1,
                          overwriting if file exists (a prompt may appear)
$ command1 >! file1     # Force overwriting of existing file1 with output of command1
$ command1 >> file1     # Append standard output of command1 to file1
$ command1 2> file2     # Write error output of command1 to file2
```

- Redirecting the standard error and output (preferably to separate files) can be extremely helpful for diagnosing errors and bugs and for asking help

## Linux pipes to pass information between commands

- Aside from redirecting streams between commands and files, streams can be redirected between commands
<!-- - The Linux philosophy is to make one tool for one task -->
- So to perform more complex operations we can combine multiple tools/commands using pipes
- Pipes take the standard output from one command and direct it to the standard input of the subsequent command

```
$ <command1> | <command2> 
$ cat file1.txt | wc -l
```

## Combining commands

- Another way to combine commands is to simply run commands one after another using a semi-colon

```
$ <command1>; <command2>        # Execute command1 and then command2 (left to right)
```

- If multiple commands are dependent on each other, we can run them conditionally

```
$ <command1> && <command2>  # Double ampersand - Execute command2 only if command1 is 
                              sucessful 
$ <command1> || <command2>  # Double pipe - Execute command2 only if command1 fails 
```

## (De)compressing files with `gzip` and `gunzip`

- Raw data files and files created during analyses can be large (up to hundreds of GB)
- Compressing files is an efficient way to save disk space

- You can compress your files using **gzip** to save disk space

  ```
  $ gzip filename.fq
  ```

  + This will add a .gz suffix to the existing file e.g. filename.fq.gz

- To decompress the gzipped file, use **gunzip**

  ```
  $ gunzip filename.fq.gz
  ```

- Many programs support gzip compressed input files
  + So no need to decompress before use

## (De)compressing files: Redirecting to standard output

- The option `-c` can be used in both commands `gzip` and `gunzip` for major benefits:
  + Original files are kept unchanged i.e. not deleted
  + Compressed or decompressed output is redirected to the standard output of the command, meaning that it can be redirected to any file name (circumventing the default behaviour of both commands)
  
```
$ gzip -c file1.txt > compressed.txt.gz
```

## File archiving with `tar`

- `tar` stands for tape archive, an archiving file format
- A tar archive combines multiple files and directories into a single file 
- Optionally, tar archives can be further compressed during their creation
<!-- - `tar` creates, modifies and extracts files that are archived in the tar format -->
- Works on entire directory or a single file

  ```
  $ tar -czvf name-of-archive.tar.gz /path/to/directory-or-file/s
  ```
  + -c = **C**reate an archive
  + -z = Compress the archive with g**z**ip
  + -v = **V**erbose to display progress in the terminal while creating the archive
  + -f = To specify the **f**ile name (path) of the archive file to create

- `tar` can also be used to extract files from a tar archive

  ```
  $ tar -xzvf name-of-archive.tar.gz  # Extract files from an archive
  ```

## (De)compressing and/or archiving files

- Good practice to compress and/or archive all non-trivial text files

```
$ gzip file1 	                # Compress file1 in place (adds .gz file extension)
$ gunzip file1.gz 	            # Decompress file1 in place (removes .gz file 
                                  extension)
$ gunzip -c file1.gz > f1.txt   # Decompress file1 to standard out (can be 
                                  redirected to a file)
$ zcat file1.gz                 # Print compressed file to the terminal
$ zless file1.gz                # Interactively scroll through compressed files 
                                  (equivalent to less)
$ tar -czf jpg.tar.gz *.jpg     # Create a compressed archive from multiple files
$ tar -xzf jpg.tar.gz           # Extract files from an archive
```

# More commands

## Searching within files using ``grep``

:::: {.columns}

::: {.column width="50%"}

- Search files and print only lines that match a given pattern
- Line-based i.e. returns all lines that match the pattern
- Regular expressions are used to encode pattern
<!-- - Pattern to search for must be given as a regular expression (type of syntax to describe patterns) -->
<!--   + Does not always need to include special wildcard characters -->
<!--   + Can be as simple as the exact sequence of characters to search for -->
  
```
$ grep <options> <pattern> <file>

# Line starting with "error"
$ grep –i “error” pipeline.log

$ grep “^error” pipeline.log     
$ grep –c “chr1” p300.bed
$ grep –v “chr5” p300.bed
```

:::

::: {.column width="50%"}

```{r}
#| fig-align: center
#| out-height: 375px
#| out-width: 500px
## Source: 
knitr::include_graphics("img/grep_options.png")
```

:::

::::

## Extracting columns from files with `cut`

- Extract one or more columns from a file

```
$ cut -f3 file1.tsv             # Extract only the third field from a tab 
                                  delimited file
$ cut –f1,4 –d ‘,’ file2.csv    # Extract the first and fourth columns from 
                                  a comma delimited file
                                # -d delim, default delimiter is tab (\t)
```

## Sorting files with `sort`

- Sort the lines in a file according to the values of one or more columns in each line

```
$ sort file1.txt file2.txt file3.txt > sorted.txt       #
$ sort -t, --key=2 --key=1n file1.txt
```
-
  + -t - Defines the delimiter that is used to separate columns
  + \-\-key - Declares one or more fields i.e. columns to use for sorting 
          - Sort rarely used without this option i.e. whole line is used for sorting
  + \-\-key=2 - Second field should be used to order lines, in alphabetical order of that field
  + \-\-key=1n - First field should be used to break ties, in numerical order of that field
  
<!-- By default, if keys are not given, sort uses entire lines for comparison -->

## Removing duplicate rows with `uniq`

- Remove duplicated lines, or count the number of occurrences of each distinct line in a file

```
$ uniq file1.txt                     # Remove duplicate lines, display on screen
$ uniq –c file1.txt                  # Prefix lines by the number of occurrences
$ uniq –u file1.txt > uniqlines.txt  # Only print unique lines
$ uniq –i file1.txt                  # Ignore differences in case when comparing
```
- Assumes that the file is sorted and will only detect adjacent duplicate lines

```
$ sort file1.txt | uniq
```

## Searching for files with `find`

<br>

```{r}
#| fig-align: center
#| out-height: 300px
#| out-width: 750px
## Source: 
knitr::include_graphics("img/search_find.png")
```

# Talk 3 Exercise 1

<!-- @sec-Talk3Exercise1 -->

## Talk 3 Exercise 1 - Working with files and streams {#sec-Talk3Exercise1}

1. Copy compressed tar archive **dnase1_new.tar.gz** from resources directory to your **1_linux_intro** directory.

```{bash, eval = FALSE}
cd <course working dir>/1_linux/1_linux_intro
cp <resources dir>/1_linux/1_linux_intro/dnase1_new.tar.gz .
```

2. Decompress and extract the archive using `tar`. Check if successful. 

```{bash, eval = FALSE}
tar -xzf dnase1_new.tar.gz      # There will be a directory named 'bed'
```

3. Move into the **bed** directory and count the number of lines in each **.bed** file.

```{bash, eval = FALSE}
cd bed
wc -l *.bed
```

4. Print the first 5 lines of the file **cpg.bed**. Compress the **cpg.bed** file in place using `gzip` and then head the compressed file. What do you notice about the compressed file?

```{bash, eval = FALSE}
head -n 5 cpg.bed
gzip cpg.bed
head cpg.bed.gz
```

5. Decompress the **cpg.bed.gz** file (generated from 4) in place using `gunzip`.

```{bash, eval = FALSE}
gunzip cpg.bed.gz
ls -l
head cpg.bed
```

6. Extract all the lines in **cpg.bed** containing regions on chromosome 5 to a new file called **cpg_chr5.bed**.

```{bash, eval = FALSE}
grep chr5 cpg.bed > cpg_chr5.bed
```

7. Extract all of the entries EXCEPT those on chr5 to another file called **cpg_nochr5.bed**.

```{bash, eval = FALSE}
grep -v chr5 cpg.bed > cpg_nochr5.bed
```

How can you check that this command has worked?

```{bash, eval = FALSE}
wc -l cpg*.bed      # Check if sum of cpg_chr5.bed and cpg_nochr5.bed lines 
                    # equal to cpg.bed lines 
```

8. With the **cpg.bed** file count how many intervals (rows) are from each chromosome (column1).
  Hint: you will need to pipe together 3 commands.

```{bash, eval = FALSE}
cut -f 1 cpg.bed | sort | uniq -c
```

## Talk 3 Exercise 1 - Working with files and streams

9. Use `find` to find all files in your course working directory with 'dnase' in their names. How many are there?

```{bash, eval = FALSE}
find <course working dir> -type f -name '*dnase*' | wc -l
```

10. Find all the **.bed** files in your course working directory and count the number of lines in each file.

```{bash, eval = FALSE}
find <course working dir> -type f -name '*.bed' -exec wc -l {} \;
```

# Reminder

- Please download the [FileZilla Client](https://filezilla-project.org/download.php?platform=osx) and/or [Cyberduck](https://cyberduck.io/download/) for tomorrow's lecture

- These programs are used for transferring files between your local machine and a remote machine (e.g. OBDS server)

# Bonus topics

## Using octal notation for file permissions

- Octal notation is more compact but takes more practice to read and write
- Each type of permission is assigned a numerical value
  + **4** for **read** permission
  + **2** for **write** permission
  + **1** for **execute** permission
- We sum the numbers for each category to give a single unique number for each combination of permissions
- Each permissions category (owner, group, other) is represented by a number between **0** (no permission) and **7** (all permission)

## Changing file permissions using octal notation

- We can change file permissions with ``chmod`` using octal notation

  ```
  $ chmod 700 file1.txt
  $ ls -l file1.txt
  -rwx------ 1 dsims obds 2981 Apr 26  2021 file1.txt
  ```

- File permissions can be displayed in octal format using ``stat``
  + ``stat`` displays detailed information about files or file systems
  + -c - use the specified format instead of the default

  ```
  stat -c "%a: %n" *              # %a: access rights in octal, %n: file name
  755: jupyter_notebooks
  644: mm10.blacklist.bed.gz
  ```

## Loops in Bash

- Iterate over lists of values, inputs, files or directories
- When repeatedly executing the same set of commands on a series of inputs, loops can be used to:
  + Define a set of input
  + Define a set of commands
  + Execute the set of commands on the set of inputs

:::: {.columns}

::: {.column width="33%"}

- Iterate over fixed inputs 

```
$
for i in 2 5 3
do
  echo "start"
  echo "value of i: '$i'"
  echo "end"
done
```

:::

::: {.column width="33%"}

- Iterate over integers

```
$
for i in {1..3}
do
  echo "value of i: '$i'"
done
```

:::

::: {.column width="33%"}

- Iterate over files

```
$
for file in *.txt
do
  wc -l $file
done
```

:::

::::

## Loops in Bash

- Tips
  + When writing a new loop, consider testing it on a small set of inputs before executing it on the full set of inputs
  + Consider adding commands that display informative messages during the execution of the loop e.g. using `echo` to print messages on stdout
  
# Bonus exercises

## Bonus exercises - Octal permissions

1. Look up the usage of the `stat` command.

```{bash, eval = FALSE}
stat --help
man stat
```

2. Display the octal permission/s for file/s in your copy of **1_linux/1_linux_intro** directory, showing only permissions, file names and total size in bytes of files. 

```{bash, eval = FALSE}
stat -c "%a: %n - %s bytes" *
```

3. Create a new text file called **test.txt** in your **1_linux_intro** and set the file permissions using octal notation to allow read, write and execute access only for you. 

```{bash, eval = FALSE}
cd <course working dir>/1_linux/1_linux_intro
touch test.txt
chmod 700 test.txt
```

## Bonus exercises - Loops

1. Create files containing subsets of cpg.bed for chromosomes, chr20, chr21 and chr22.

```{bash, eval = FALSE}
cd <course working dir>/1_linux/1_linux_intro/bed

for chr in chr20 chr21 chr22
do
    grep ${chr} cpg.bed > cpg_${chr}.bed
done
```

## References

```{r}
#| results: asis
PrintBibliography(bib)
```
