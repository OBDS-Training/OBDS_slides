# Visually extracting information from data

```{r}
allen <- ReprocessedAllenData()
allen <- scater::logNormCounts(x = allen, exprs_values = "tophat_counts")
allen <- scater::runPCA(allen)
allen <- scater::runUMAP(allen)
```

.pull-left[
## Data

<br/>

.x-small-table[
```{r, include=TRUE, echo=FALSE}
nrow <- 10
ncol <- 5
rownames <- paste("gene", seq_len(nrow))
colnames <- paste("sample", seq_len(ncol))
x <- matrix(data = rbinom(nrow*ncol, 10E3, 1E-4), nrow = nrow, ncol = ncol, dimnames = list(rownames, colnames))
knitr::kable(x, format = "html", escape = F)
```
]
]

.pull-right[
## Information

```{r, include=TRUE, echo=FALSE}
x <- log(assay(allen, "tophat_counts") + 1)
keep_rows <-  head(order(rowVars(x), decreasing = TRUE), 50)
set.seed(10)
keep_columns <- sample(ncol(x), 20)
Heatmap(
  matrix = x[keep_rows, keep_columns],
  row_names_gp = gpar(fontsize = 8))
```
]

---

# Sources of variation in data


Difference in data (e.g., expression) can come from multiple sources:

- Biological

- Technical

Either of those sources could be either:

- Of interest to study

- Considered a confounding covariate

.center[
**Signal and noise both depend on your research question.**
]

---

# Confounding

Experimental design is crucial to ensure that sources of interesting variation are not confounded with independent sources of uninteresting variation (e.g. technical).

.pull-left[
## Confounded

<br/>

.small-table[
```{r, include=TRUE, echo=FALSE}
nrow <- 8
data.frame(
  Cell = seq_len(nrow),
  Site = rep(c("S1", "S2"), each = 4),
  Treatment = rep(c("A", "B"), each = 4)
) %>%
  mutate(
    Site = cell_spec(Site, "html", color = ifelse(Site == "S2", "red", "blue")),
    Treatment = cell_spec(Treatment, "html", color = ifelse(Treatment == "A", "cyan", "orange"))
  ) %>% 
  knitr::kable(format = "html", escape = F)
```
]
]

.pull-left[
## Balanced

<br/>

.small-table[
```{r, include=TRUE, echo=FALSE}
nrow <- 8
data.frame(
  Cell = seq_len(nrow),
  Site = rep(c("S1", "S2"), each = 4),
  Treatment = rep(c("A", "B"), times = 4)
) %>%
  mutate(
    Site = cell_spec(Site, "html", color = ifelse(Site == "S2", "red", "blue")),
    Treatment = cell_spec(Treatment, "html", color = ifelse(Treatment == "A", "cyan", "orange"))
  ) %>% 
  knitr::kable(format = "html", escape = F)
```
]
]

---

# Feature selection

Many genes are not interesting because they don't vary much, or they
donâ€™t have enough counts.

Filtering for feature selection is needed to:

- Select genes that display useful variation.

- Reduce memory usage and computational cost/time.

```{r, include=TRUE, echo=FALSE, fig.align='center', fig.height=4, fig.width=6}
set.seed(1)
mat <- matrix(c(rnorm(12*4, 0, 10), rnorm(12*4, 0, 1)), nrow = 8, ncol = 12, byrow = TRUE)
ComplexHeatmap::Heatmap(mat, name = "values")
```

---

# Dimensionality reduction

We use dimensionality reduction methods to:

- Find structure in the data.

- Aid in visualization.

Unsupervised learning helps finding groups of homogeneous items

- Many approaches to do this (e.g. PCA, t-SNE, UMAP)

.pull-left[
.x-small-table[
```{r, include=TRUE, echo=FALSE}
nrow <- 10
ncol <- 5
rownames <- paste("gene", seq_len(nrow))
colnames <- paste("sample", seq_len(ncol))
x <- matrix(data = rbinom(nrow*ncol, 10E3, 1E-4), nrow = nrow, ncol = ncol, dimnames = list(rownames, colnames))
knitr::kable(x, format = "html", escape = F)
```
]
]

.pull-right[
.x-small-table[
```{r, include=TRUE, echo=FALSE}
nrow <- 5
ncol <- 2
rownames <- paste("sample", seq_len(nrow))
colnames <- paste("dim", seq_len(ncol))
x <- matrix(data = rnorm(nrow*ncol, 0, 10), nrow = nrow, ncol = ncol, dimnames = list(rownames, colnames))
knitr::kable(x, format = "html", escape = F)
```
]

]

---

# Principal component analysis (PCA)

.pull-left[
## Goals

- Find linear combination of variables to create principal components (PCs).
- Maintain most variance in the data (for given number of PCs).
- PCs are uncorrelated (orthogonal to each other) and ordered with respect to the percentage of variance explained.

## Assumptions
- Relationship between variables is linear!
- Not optimal for non-linear data structures.
]

.pull-right[
<br/>

```{r, include=TRUE, echo=FALSE}
plotReducedDim(object = allen, dimred = "PCA", colour_by = "driver_1_s")
```
]

.center[
```{r, include=TRUE, eval=FALSE}
pca <- prcomp(x, center = TRUE, scale. = FALSE, ...)
```
]

---

# PCA example

.pull-left[
```{r, include=TRUE, echo=FALSE}
set.seed(24)
df <- data.frame(
  x = rnorm(n = 100, mean = 0, sd = 1)
) %>% 
  mutate(
  y = x + rnorm(n = 100, mean = 0, sd = 0.8)
  )
ggplot(df) +
  geom_point(aes(x, y)) +
  labs(x = "gene 1", y = "gene 2")
```
]

--

.pull-right[
```{r, include=TRUE, echo=FALSE}
pca <- prcomp(df)
df_eigengenes <- as_tibble(t(pca$rotation * rep(pca$sdev^2, each=2))) %>% 
  mutate(
    xend = 0,
    yend = 0,
    PC = paste0("PC", 1:2))
ggplot() +
  geom_point(aes(x, y), df) +
  geom_segment(
    aes(x = xend, y = yend, xend=x, yend=y, color = PC), df_eigengenes,
    size=1.25,
    arrow = arrow(length = unit(10, "points"), angle = 30)) +
  coord_cartesian(xlim = range(df$x), ylim = range(df$y)) +
  labs(x = "gene 1", y = "gene 2")
```
]

$$PC1 = \beta_{(1,1)} * gene_1 + \beta_{(1,2)} * gene_2$$
$$PC2 = \beta_{(2,1)} * gene_1 + \beta_{(2,2)} * gene_2$$

---

# Eigenvalue decomposition

Eigenvalue decomposition is matrix factorization algorithm.

```{r, include=TRUE, echo=FALSE, fig.align="center"}
## Source: https://notes.andrewgurung.com/data-science/linear-algebra/eigenvalues-and-eigenvectors
knitr::include_graphics("img/eigen_decomposition.png")
```

In the context of PCA:

- An eigenvector represents a direction or axis.
- The corresponding eigenvalue represents variance along that eigenvector.

---

# PCA

- First, center data.
  + Always best, unless you have a good reason not to.

- If comparing different units, scale data.
  + i.e., using correlation matrix instead of covariance matrix<sup>1</sup>
  + Genes have very different dynamic ranges!

--

.pull-left[
**Spectral decomposition = Eigen decomposition**
- More intuitive, but computationally slower
]

.pull-right[
**Singular Value Decomposition (SVD)**
- Equivalent, faster
]

--

**Approach:**

- The idea is to select a smaller number of dimensions by taking the first $k$ out of $n$
eigenvectors that explain as much of the variability of the data as possible.
- How to choose k?

**See also:** [Towards data science](https://towardsdatascience.com/let-us-understand-the-correlation-matrix-and-covariance-matrix-d42e6b643c22), "Correlation matrix and covariance matrix"

---

# Expression data example

Airway smooth muscle cells expression profiling by high throughput sequencing; [GSE52778](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE52778).

```{r}
data(airway)
```

.pull-left[
```{r, include=TRUE, echo=FALSE, fig.height=4, fig.width=8}
keep_rows <- rowMeans(assay(airway, "counts")) > 1
assay(airway, "counts")[keep_rows, ] %>% 
  melt(varnames = c("gene", "sample"), value.name = "count") %>%
  as_tibble() %>% 
  ggplot() +
  geom_density(aes(log10(count), group = sample)) +
  labs(
    x = "Raw read counts per gene (log10) - Per sample", y = "Density"
  ) +
  theme_cowplot()
```

```{r, include=TRUE, echo=FALSE, fig.height=4, fig.width=8}
tibble(
  rowSums = rowSums(assay(airway, "counts")[keep_rows, ])
) %>% 
  ggplot() +
  geom_histogram(aes(rowSums), bins = 100, color = "black", fill = "grey") +
  scale_x_log10() +
  labs(
    x = "Sum of raw read counts (log10) - Per gene", y = "Frequency"
  ) +
  theme_cowplot()
```
]

.pull-right[
* `dex`: treatment with dexamethasone
* `cell`: cell line

```{r, include=TRUE, echo=FALSE, fig.height=8, fig.width=8}
keep_rows <- log10(assay(airway, "counts") + 1) %>% 
  rowVars() %>% 
  order(decreasing = TRUE) %>% 
  head(50)
dex_col <- head(brewer.pal(2, "Set1"), 2); names(dex_col) <- levels(airway$dex)
cell_col <- head(brewer.pal(4, "Set3"), 4); names(cell_col) <- levels(airway$cell)
ha_top <- columnAnnotation(
  df = colData(airway) %>% as_tibble() %>% dplyr::select(dex, cell) %>% as.data.frame(),
  col = list(dex = dex_col, cell = cell_col),
  simple_anno_size = unit(1, "cm"))
hm <- Heatmap(
  matrix = log10(assay(airway, "counts")[keep_rows, ] + 1),
  name = "Raw counts\n(log10)\n",
  row_names_gp = gpar(fontsize = 8),
  top_annotation = ha_top)
draw(hm)
```
]

---

# Expression data example

Airway smooth muscle cells expression profiling by high throughput sequencing; [GSE52778](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE52778).

```{r}
data(airway)
```

```{r}
keep_rows <- log10(assay(airway, "counts") + 1) %>% 
  rowVars() %>% 
  order(decreasing = TRUE) %>% 
  head(500)
pca <- log10(assay(airway, "counts") + 1)[keep_rows, ] %>% 
  t() %>% 
  prcomp()
scree_table <- tibble(
  sdev = pca$sdev,
  PC = seq_along(pca$sdev),
  var = sdev^2 / sum(sdev^2),
  var_cumsum = cumsum(var)
)
```

.pull-left[
```{r, include=TRUE, echo=FALSE, fig.height=5}
keep_samples <- rownames(pca$x)
pca$x %>%
  as_tibble() %>% 
  dplyr::select(PC1, PC2) %>% 
  bind_cols(colData(airway)[keep_samples, ] %>% as_tibble()) %>% 
  ggplot(aes(PC1, PC2, color = cell, shape = dex)) +
  geom_point(size = 3) +
  labs(
    x = sprintf("PC1 (%.2f %%)", 100*subset(scree_table, PC == 1, "var")),
    y = sprintf("PC2 (%.2f %%)", 100*subset(scree_table, PC == 2, "var"))
  )
```

Percentage variance explained:

$$pct\_var = sdev^2 / sum(sdev^2)$$
]

.pull-right[
```{r, include=TRUE, echo=FALSE, fig.height=4}
ggplot(scree_table, aes(PC, var)) +
  geom_col(fill = "grey") +
  geom_point() +
  scale_x_continuous(breaks = seq_along(scree_table$PC)) +
  labs(y = "% Variance explained", title = "Percentage of Variance Explained")
```

```{r, include=TRUE, echo=FALSE, fig.height=4}
ggplot(scree_table, aes(PC, var_cumsum)) +
  geom_line() + geom_point() +
  scale_x_continuous(breaks = seq_along(scree_table$PC)) +
  labs(y = "Cumulative % Variance explained", title = "Cumulative Percentage of Variance Explained")
```
]

---

# PCA - Loadings / Rotation matrix

```{r, include=TRUE,echo=TRUE}
# Visualise loadings for the first five genes and principal components
pca$rotation[1:5, 1:5]
```

Meaning that for each cell:

$PC1_{(cell)} = 0.1255530 \times ENS00000129824_{(cell)}\ - 0.1293194 \times\ ...$

---

# Visualize top genes

Airway smooth muscle cells expression profiling by high throughput sequencing; [GSE52778](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE52778).

## Top / Bottom loadings

```{r}
data(airway)
```

```{r}
keep_rows <- log10(assay(airway, "counts") + 1) %>% 
  rowVars() %>% 
  order(decreasing = TRUE) %>% 
  head(500)
pca <- log10(assay(airway, "counts") + 1)[keep_rows, ] %>% 
  t() %>% 
  prcomp()
```

.pull-left[
```{r, include=TRUE, echo=FALSE, fig.height=6}
keep_pc <- "PC1"
bind_rows(
  pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 10, wt = loading) %>% 
  mutate(direction = "+"),
  pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 10, wt = -loading) %>% 
  mutate(direction = "-")
) %>% 
  mutate(
    gene = reorder(gene, loading, identity),
    direction = factor(direction, c("-", "+"))
  ) %>% 
  ggplot(aes(gene, loading)) +
  geom_col(aes(fill = direction), show.legend = FALSE) +
  labs(x = NULL, title = keep_pc) +
  theme_cowplot() +
  theme(
    axis.text.x = element_text(angle = 90)
  )
```
]

.pull-right[
```{r, include=TRUE, echo=FALSE, fig.height=6}
keep_pc <- "PC2"
bind_rows(
  pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 10, wt = loading) %>% 
  mutate(direction = "+"),
  pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 10, wt = -loading) %>% 
  mutate(direction = "-")
) %>% 
  mutate(
    gene = reorder(gene, loading, identity),
    direction = factor(direction, c("-", "+"))
  ) %>% 
  ggplot(aes(gene, loading)) +
  geom_col(aes(fill = direction), show.legend = FALSE) +
  labs(x = NULL, title = keep_pc) +
  theme_cowplot() +
  theme(
    axis.text.x = element_text(angle = 90)
  )
```
]

---

# Visualize top genes - expression

.pull-left[
## PC1

```{r, include=TRUE, echo=FALSE, fig.height=3.5}
keep_pc <- "PC1"
keep_gene <- pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 1, wt = loading) %>% 
  pull(gene) %>% as.character()
assay(airway, "counts")[keep_gene, , drop=FALSE] %>% 
  melt(varnames = c("gene", "sample"), value.name = "counts") %>% 
  bind_cols(colData(airway) %>% as_tibble()) %>% 
  ggplot(aes(dex, counts, color=dex)) +
  geom_boxplot(width = 0.5) +
  geom_point() +
  scale_y_log10() +
  labs(title = keep_gene, y = "counts (log-scale)") +
  theme_cowplot()
```

```{r, include=TRUE, echo=FALSE, fig.height=3.5}
keep_pc <- "PC1"
keep_gene <- pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 1, wt = loading) %>% 
  pull(gene) %>% as.character()
assay(airway)[keep_gene, , drop=FALSE] %>% 
  melt(varnames = c("gene", "sample"), value.name = "counts") %>% 
  bind_cols(colData(airway) %>% as_tibble()) %>% 
  ggplot(aes(cell, counts, color=cell)) +
  geom_boxplot(width = 0.5) +
  geom_point() +
  scale_y_log10() +
  labs(title = keep_gene, y = "counts (log-scale)") +
  theme_cowplot()
```
]

.pull-right[
## PC2

```{r, include=TRUE, echo=FALSE, fig.height=3.5}
keep_pc <- "PC2"
keep_gene <- pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 1, wt = loading) %>% 
  pull(gene) %>% as.character()
assay(airway)[keep_gene, , drop=FALSE] %>% 
  melt(varnames = c("gene", "sample"), value.name = "counts") %>% 
  bind_cols(colData(airway) %>% as_tibble()) %>% 
  ggplot(aes(dex, counts, color=dex)) +
  geom_boxplot(width = 0.5) +
  geom_point() +
  scale_y_log10() +
  labs(title = keep_gene, y = "counts (log-scale)") +
  theme_cowplot()
```

```{r, include=TRUE, echo=FALSE, fig.height=3.5}
keep_pc <- "PC2"
keep_gene <- pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 1, wt = loading) %>% 
  pull(gene) %>% as.character()
assay(airway)[keep_gene, , drop=FALSE] %>% 
  melt(varnames = c("gene", "sample"), value.name = "counts") %>% 
  bind_cols(colData(airway) %>% as_tibble()) %>% 
  ggplot(aes(cell, counts, color=cell)) +
  geom_boxplot(width = 0.5) +
  geom_point() +
  scale_y_log10() +
  labs(title = keep_gene, y = "counts (log-scale)") +
  theme_cowplot()
```
]

---

# Visualize top genes - expression

.pull-left[
## ggplot2::geom_tile()

```{r, include=TRUE, echo=FALSE, fig.height=8}
keep_pc <- "PC1"
keep_rows <- pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 20, wt = loading) %>% 
  pull(gene) %>%
  as.character()
assay(airway)[keep_rows, , drop=FALSE] %>%
  t() %>%
  as_tibble() %>%
  bind_cols(colData(airway) %>% as_tibble()) %>%
  pivot_longer(
    cols = starts_with("ENS"),
    names_to = "gene", values_to = "counts"
  ) %>% 
  ggplot() +
  geom_tile(aes(dex, gene, fill = log10(counts + 1))) +
  facet_wrap(~cell, nrow = 1) +
  labs(fill = "Raw counts\n(log10)\n") +
  scale_fill_viridis_c()
```
]

.pull-right[
## ComplexHeatmap::Heatmap()

```{r, include=TRUE, echo=FALSE, fig.height=8}
keep_pc <- "PC1"
keep_rows <- pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 20, wt = loading) %>% 
  pull(gene) %>%
  as.character()
dex_col <- head(brewer.pal(2, "Set1"), 2); names(dex_col) <- levels(airway$dex)
cell_col <- head(brewer.pal(4, "Set3"), 4); names(cell_col) <- levels(airway$cell)
ha_top <- columnAnnotation(
  df = colData(airway) %>% as.data.frame() %>% dplyr::select(dex, cell),
  col = list(dex = dex_col, cell = cell_col),
  simple_anno_size = unit(1, "cm"))
hm <- Heatmap(
  matrix = log10(assay(airway, "counts")[keep_rows, ] + 1),
  name = "Raw counts\n(log10)\n",
  row_names_gp = gpar(fontsize = 8),
  top_annotation = ha_top)
draw(hm)
```
]

---

# Visualize top genes - expression

.pull-left[
## PC1

```{r, include=TRUE, echo=FALSE, fig.height=5}
keep_pc <- "PC1"
keep_loading_table <- pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 1, wt = abs(loading))
keep_gene <- keep_loading_table %>% pull(gene) %>%  as.character()
keep_samples <- rownames(pca$x)
assay(airway)[keep_gene, keep_samples, drop=FALSE] %>% 
  melt(varnames = c("gene", "sample"), value.name = "counts") %>% 
  bind_cols(pca$x %>% as_tibble() %>% dplyr::select(PC1, PC2)) %>% 
  ggplot(aes(PC1, PC2, color=log10(counts + 1))) +
  geom_point(size = 3) +
  scale_color_viridis_c() +
  labs(
    title = sprintf("%s", keep_gene),
    subtitle = sprintf(
      "Loading: %.3f",
      keep_loading_table %>% filter(gene == keep_gene) %>% pull(loading))) +
  theme_cowplot()
```
]

.pull-right[
## PC2

```{r, include=TRUE, echo=FALSE, fig.height=5}
keep_pc <- "PC2"
keep_loading_table <- pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 1, wt = abs(loading))
keep_gene <- keep_loading_table %>% pull(gene) %>%  as.character()
keep_samples <- rownames(pca$x)
assay(airway)[keep_gene, keep_samples, drop=FALSE] %>% 
  melt(varnames = c("gene", "sample"), value.name = "counts") %>% 
  bind_cols(pca$x %>% as_tibble() %>% dplyr::select(PC1, PC2)) %>% 
  ggplot(aes(PC1, PC2, color=log10(counts + 1))) +
  geom_point(size = 3) +
  scale_color_viridis_c() +
  labs(
    title = sprintf("%s", keep_gene),
    subtitle = sprintf(
      "Loading: %.3f",
      keep_loading_table %>% filter(gene == keep_gene) %>% pull(loading))) +
  theme_cowplot()
```
]

---

# Exercise

## Setup

- Import the `iris` data set.

- Separate the matrix of measurements in a new object named `iris_features`.

---

# Exercise

## Apply Principal Components Analysis (PCA)

The `prcomp()` function allows you to standardise the data as part of the principal components analysis itself.

- Apply PCA while centering and scaling the matrix of features.

- Examine the PCA output.
  Display the loading of each feature on each principal component.

Use the return value of the PCA to create a `data.frame` called `pca_iris_dataframe` that contains the coordinates projected on principal components.

- Visualise the PCA projection using `ggplot2::geom_point()`.

### Bonus point

- Color data points according to their class label.

Save the PCA plot as an object named `pca_iris_species`.

---

# Exercise

## Variable loading

- Color a scatter plot of PC1 and PC2 by the value of the variable most strongly associated with the first principal component.

What do you observe?

## Variance explained

- Compute the variance explained by principal components, using information present in the return value of the `prcomp()` function.

- Visualise the variance explained by each principal component using `ggplot2::geom_col()`.

---

# Exercise

## UMAP

- Apply UMAP on the output of the PCA.

- Inspect the UMAP output.

- Visualise the UMAP projection using `ggplot2::geom_point()`.

### Bonus point

- Color data points according to their class label.

Save the UMAP plot as an object named `umap_plot`.

---

# t-SNE

.center[**t-Distributed Stochastic Neighbor Embedding**]

- Technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.
- Aims to place cells with similar local neighbourhoods in high-dimensional space together in low-dimensional space.
- Non-linear dimensionality reduction (as opposed to PCA).
- R implementation https://lvdmaaten.github.io/tsne/

.pull-left[
- Preserve local structure / small pairwise distances / local similarities 
]

.pull-right[
```{r, include=TRUE, echo=FALSE}
knitr::include_graphics("https://miro.medium.com/max/260/1*vNMeKlFkTs9gb_6B93s-yA.png")
```

]

.footnote[
**See also:**

1. [Towards data science](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1), "An Introduction to t-SNE with Python Example".
]

---

# t-SNE

Finds a way to project data into a low-dimension space (here, 1-D line), so that the clustering in the high-dimension space (here, 2-D scatter plot) is preserved.

```{r, include=TRUE, echo=FALSE, fig.align="center", out.height="300px"}
knitr::include_graphics("https://younesse.net/assets/Slides/Visualization/tSNE_step0.png")
```

.footnote[
**See also:**

1. [StatQuest](https://statquest.org/statquest-t-sne-clearly-explained/), "t-SNE, clearly explained!".
2. [younesse.net](https://younesse.net/assets/Slides/Visualization/Visualization.html), "Dimensionality reduction & visualization of representations".
]

---

# Expression data example

Airway smooth muscle cells expression profiling by high throughput sequencing; [GSE52778](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE52778).

.pull-left[
## PCA

```{r, include=TRUE, echo=FALSE, fig.height=5}
as_tibble(pca$x) %>%
  dplyr::select(PC1, PC2) %>% 
  mutate(Sample = rownames(pca$x)) %>% 
  bind_cols(colData(airway)[rownames(pca$x), c("cell", "dex")] %>% as_tibble()) %>% 
  ggplot(aes(PC1, PC2, color = cell, shape = dex)) +
  geom_point(size = 5) +
  labs(
    x = sprintf("PC1 (%.2f %%)", 100*subset(scree_table, PC == 1, "var")),
    y = sprintf("PC2 (%.2f %%)", 100*subset(scree_table, PC == 2, "var"))
  ) +
  theme_cowplot()
```
]

.pull-right[
## t-SNE

```{r, include=TRUE, echo=FALSE, fig.height=5, fig.align='center'}
set.seed(10)
tsne_out <- Rtsne(X = pca$x, perplexity = 1)
tsne_out$Y %>% 
  as_tibble() %>% 
  mutate(Sample = rownames(pca$x)) %>% 
  bind_cols(colData(airway)[rownames(pca$x), c("cell", "dex")] %>% as_tibble()) %>% 
  ggplot(aes(V1, V2, color = cell, shape = dex)) +
  geom_point(size = 5) +
  labs(x = "t-SNE 1", y = "t-SNE 2") +
  theme_cowplot()
```
]

---

# Exercise

## t-SNE

- Apply t-SNE and inspect the output.

- Use the return value of the t-SNE to create a `data.frame` called `tsne_iris_dataframe` that contains the coordinates.

- Visualise the t-SNE projection.

### Bonus points

- Color data points according to their class label.

Save the t-SNE plot as an object named `tsne_iris_species`.

- Combine PCA, UMAP and t-SNE plots in a single figure.

---

# Clustering

- Technique for grouping of given data points and classification into groups.
  + In theory, points with similar properties or features should belong to the same group.
  + Points with dissimilar properties or features should belong to different groups.
  + Method of unsupervised learning (no known labels), common technique used in many fields.
- Yields valuable insights from data by seeing what groups fall into after clustering
- Many methods (e.g. K-means clustering, hierarchical clustering)

.pull-left[
```{r, include=TRUE, echo=FALSE, fig.align="center", fig.height=6}
ggplot(iris, aes(Sepal.Length, Sepal.Width)) +
    geom_point() +
    cowplot::theme_cowplot() +
    labs(x = "variable 1", y = "variable 2")
```
]

.pull-right[
```{r, include=TRUE, echo=FALSE, fig.align="center", fig.height=6}
iris %>% 
    mutate(
        Cluster = Species %>% 
            fct_recode(
                "1" = "setosa",
                "2" = "versicolor",
                "3" = "virginica"
            )
    ) %>% 
    ggplot(aes(Sepal.Length, Sepal.Width, color = Cluster)) +
    geom_point() +
    cowplot::theme_cowplot() +
    labs(x = "variable 1", y = "variable 2")
```
]

---

# K-Means Clustering

- Probably the most well known clustering algorithm (unsupervised).
- Easy to understand and implement.

.pull-left[
## Pros

- Very fast
]

.pull-right[
## Cons

- Need to preselect the number of groups/classes â€“ not always trivial
- Random choice of cluster centers can yield different clustering results on different attempts.
]

---

# K-means clustering - Iterations

1. Initialise $k$ centroids randomly.
2. Assign each data points to the nearest centroid.
3. Compute new centroid coordinates.
4. Repeat (2) and (3) until convergence, or for a maximum number of iterations allowed.

```{r, include=TRUE, echo=FALSE, fig.align="center"}
knitr::include_graphics("https://stanford.edu/~cpiech/cs221/img/kmeansViz.png")
```

---

# K-Means Clustering - How many clusters?

```{r, include=TRUE, echo=FALSE, fig.align="center"}
set.seed(10)
keep_rows <-  head(order(rowVars(log10(assay(airway, "counts") + 1)), decreasing = TRUE), 500)
keep_samples <- colnames(airway)
airway_counts <- log10(assay(airway, "counts") + 1)[keep_rows, keep_samples]
airway_counts %>%
  t() %>% 
  as_tibble() %>% 
  mutate(
    "k = 1" = as.factor(NA),
    "k = 2" = as.factor(kmeans(x = t(airway_counts), centers = 2)$cluster),
    "k = 4" = as.factor(kmeans(x = t(airway_counts), centers = 4)$cluster),
    "k = 6" = as.factor(kmeans(x = t(airway_counts), centers = 6)$cluster)
  ) %>% 
  dplyr::select(starts_with("k = ")) %>% 
  bind_cols(
    pca$x[keep_samples, c("PC1", "PC2")] %>% as_tibble()) %>% 
  pivot_longer(cols = starts_with("k ="), names_to = "k", values_to = "cluster") %>% 
  ggplot(aes(PC1, PC2, color = cluster)) +
  geom_point(size = 3) +
  facet_wrap(~k) +
  theme_cowplot()
```

---

# K-means clustering

To choose $k$, run multiple values and try to maximise `betweenss` / `totss`, which is a measure of how well the data is clustered.

- **Sum of squares between clusters:** how far points are _between_ clusters (separation).
- **Sum of squares within clusters:** how close points are _within_ clusters (compactness).

For good clustering we want small `sum(withinss)` and large `betweenss`, so this ratio we want to be as large as possible.

```{r}
kmeans_scree <- lapply(seq_len(nrow(pca$x)-1), function(x) {
  out <- kmeans(x = t(airway_counts), centers = x)
  tibble(
    k = x,
    totss = out$totss,
    betweenss = out$betweenss,
    withinss_sum = sum(out$withinss)
  )
}) %>% 
  bind_rows()
```

.pull-left[
```{r, include=TRUE, echo=FALSE, fig.height=4}
kmeans_scree %>% 
  ggplot(aes(k, betweenss / totss)) +
  geom_line() +
  geom_point() +
  labs(title = "betweenss / totss") +
  scale_y_continuous(limits = c(0, 1)) +
  theme_cowplot()
```
]

.pull-right[
```{r, include=TRUE, echo=FALSE, fig.height=4}
kmeans_scree %>% 
  ggplot(aes(k, withinss_sum)) +
  geom_line(linetype = "F1") +
  geom_point() +
  labs(title = "sum(withinss)") +
  theme_cowplot()
```
]

---

# Hierarchical clustering

- Aims to build a hierarchy of classes
- To decide which clusters are similar/dissimilar, use a metric (distance between observations), e.g. Euclidean distance
- Either a bottom-up ('agglomerative') or a top-down ('divisive') approach.
  + **Agglomerative:** each cell is initially assigned to its own cluster and pairs of clusters are subsequently merged to create a hierarchy.
  + **Divisive:** starts with all observations in one cluster and then recursively split each cluster to form a hierarchy.
  
```{r, include=TRUE, echo=FALSE, out.height="300px", fig.align='center'}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/UPGMA_Dendrogram_Hierarchical.svg/1200px-UPGMA_Dendrogram_Hierarchical.svg.png")
```

---

# Hierarchical clustering

```{r}
hclust_out <- hclust(dist(t(log10(assay(airway, "counts") + 1)[keep_rows, ])))
```

.pull-left[
```{r, include=TRUE, echo=FALSE, fig.height=4, fig.align="center"}
plot(hclust_out)
```

```{r, include=TRUE, echo=FALSE, fig.height=4, fig.align="center"}
pvclust_out <- pvclust(log10(assay(airway, "counts") + 1)[keep_rows, ], quiet = TRUE)
plot(pvclust_out)
rect.hclust(hclust_out, 4)
```
]

.pull-right[
```{r, include=TRUE, echo=FALSE, fig.height=8, fig.align="center"}
keep_pc <- "PC1"
keep_rows <- pca$rotation %>%
  melt(varnames = c("gene", "PC"), value.name = "loading") %>% 
  filter(PC == keep_pc) %>% 
  top_n(n = 20, wt = loading) %>% 
  pull(gene) %>%
  as.character()
pheatmap(log10(assay(airway, "counts")[keep_rows, ] + 1))
```
]

`r BiocStyle::CRANpkg("pvclust")` (CRAN)

---

# Comparing clustering algorithms on toy datasets

```{r, include=TRUE, echo=FALSE}
## Source: https://newbedev.com/scikit_learn/modules/clustering
knitr::include_graphics("img/sphx_glr_plot_cluster_comparison_0011.png")
```

---

# Exercise

## Hierarchical clustering

- Perform hierarchical clustering on the `iris_features` data set,
  using the `euclidean` distance and method `ward.D2`.

- Plot the clustering tree.

How many clusters would you call from a visual inspection of the tree?

- Bonus point: Color leaves by known species (use `dendextend`).

- Cut the tree in 3 clusters and extract the cluster labels.

- Repeat clustering using 3 other agglomeration methods:

  + `complete`
  + `average`
  + `single`

- Compare clustering results on scatter plots of the data.

---

# Exercise

## dbscan

- Apply `dbscan` to the `iris_features` data set.

- Visualise the `dbscan` cluster label on a scatter plot of the data.

## hdbscan

- Apply `hdbscan` to the `iris_features` data set.

- Visualise the `hdbscan` cluster label on a scatter plot of the data.

## Bonus point

- Combine the plots of `dbscan` and `hdbscan` into a single plot.

---

# Exercise

## K-means

- Apply $K$-means clustering to `iris_features` with $K$ set to 3 clusters.

- Inspect the output.

- Extract the cluster labels.

- Extract the cluster centers.

- Construct a data frame that combines the `iris` dataset and the cluster label.

- Plot the data set as a scatter plot.

  + Color by cluster label.

### Bonus point

- Add cluster centers to the plot.

---

# Exercise

## Cross-tabulation with ground truth

- Cross-tabulate cluster labels with known labels.

How many observations are mis-classified by $K$-means clustering?

## Elbow plot

- Plot the "total within-cluster sum of squares" for $K$ ranging from 2 to 10.

Do you agree that 3 is the optimal number of clusters for this data set?

---

# Further reading

- `r BiocStyle::CRANpkg("dimRed")` vignette.

- [Hitchhikerâ€™s Guide to Matrix Factorization and PCA](https://aedin.github.io/PCAworkshop/index.html)

---

# Git wrap-up

- Copy your notebook for this lesson in the Git repository, e.g.

.small-code[

> ```
> cp \
>   /project/obds/$USER/projects/dimensionality_reduction_clustering/dimensionality_reduction_clustering-kevinrue.Rmd \
>   /project/obds/$USER/git/week3day2-dimensionality_reduction_clustering/
> ```

]

- Create a new branch in the Git repository.
  Use the date and your username in the branch name to make it unique, e.g. `git checkout -b week3day2-kevinrue`.

- Commit, push, open a pull request, get it approved, and merge it.

- Delete your merged branch on GitHub.

- Check out the main branch locally, pull, and delete your merged branch locally.

---

# References

.small-text[
```{r, include=TRUE, echo=FALSE, results="asis"}
PrintBibliography(bib)
```
]
