## <i class="fab fa-r-project"></i> is built for statistics

- <i class="fab fa-r-project"></i> includes a number of common statistical distributions:
  - The Normal Distribution
  - The Binomial Distribution
  - The Poisson Distribution
  - ...
- <i class="fab fa-r-project"></i> implements a range of statistical tests:
  - Student's t-Test
  - Pearson's Chi-squared Test for Count Data
  - Wilcoxon Rank Sum and Signed Rank Tests
  - ...

## <i class="fab fa-r-project"></i> Functions for Probability Distributions

:::: {.columns}

::: {.column width="60%"}
::: {.small-table}
|Distribution                   |Probability |Quantile    |Density     |Random      |
|:------------------------------|:-----------|:-----------|:-----------|:-----------|
|Beta                           |`pbeta`     |`qbeta`     |`dbeta`     |`rbeta`     |
|Binomial                       |`pbinom`    |`qbinom`    |`dbinom`    |`rbinom`    |
|Cauchy                         |`pcauchy`   |`qcauchy`   |`dcauchy`   |`rcauchy`   |
|Chi-Square                     |`pchisq`    |`qchisq`    |`dchisq`    |`rchisq`    |
|Exponential                    |`pexp`      |`qexp`      |`dexp`      |`rexp`      |
|F                              |`pf`        |`qf`        |`df`        |`rf`        |
|Gamma                          |`pgamma`    |`qgamma`    |`dgamma`    |`rgamma`    |
|Geometric                      |`pgeom`     |`qgeom`     |`dgeom`     |`rgeom`     |
|Hypergeometric                 |`phyper`    |`qhyper`    |`dhyper`    |`rhyper`    |
|Logistic                       |`plogis`    |`qlogis`    |`dlogis`    |`rlogis`    |
|Log Normal                     |`plnorm`    |`qlnorm`    |`dlnorm`    |`rlnorm`    |
|Negative Binomial              |`pnbinom`   |`qnbinom`   |`dnbinom`   |`rnbinom`   |
|Normal                         |`pnorm`     |`qnorm`     |`dnorm`     |`rnorm`     |
|Poisson                        |`ppois`     |`qpois`     |`dpois`     |`rpois`     |
|Student t                      |`pt`        |`qt`        |`dt`        |`rt`        |
|Studentized Range              |`ptukey`    |`qtukey`    |`dtukey`    |`rtukey`    |
|Uniform                        |`punif`     |`qunif`     |`dunif`     |`runif`     |
|Weibull                        |`pweibull`  |`qweibull`  |`dweibull`  |`rweibull`  |
|Wilcoxon Rank Sum Statistic    |`pwilcox`   |`qwilcox`   |`dwilcox`   |`rwilcox`   |
|Wilcoxon Signed Rank Statistic |`psignrank` |`qsignrank` |`dsignrank` |`rsignrank` |
:::
:::

::: {.column width="40%"}
- Each distribution has a root name, e.g. `norm`
- Every distribution has four functions.
- The root name is prefixed by one of the letters:
  - `p` for "probability", the cumulative distribution function (c. d. f.)
  - `q` for "quantile", the inverse c. d. f.
  - `d` for "density", the density function (p. f. or p. d. f.)
  - `r` for "random", a random variable having the specified distribution

:::

::::

::: {.notes}
Source: <https://www.stat.umn.edu/geyer/old/5101/rlook.html>
:::

## The normal distribution

### Notation

$${\mathcal{N}}(\mu ,\sigma^{2})$$

<br/>

### Parameters

- ${\mu \in \mathbb {R} }$ = mean (location)
- ${ \sigma ^{2}>0}$ = variance (squared scale)

<br/>

### Properties

:::: {.columns}

::: {.column width="50%"}
- Median: ${ \mu }$

- Mode: ${ \mu }$
:::

::: {.column width="50%"}
- Variance: ${ \sigma ^{2} }$
:::

::::

- Probability density function (PDF): ${\displaystyle {\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}}$

## Mean and standard deviation

The base R functions `mean()` and `sd()` compute the mean and standard deviation of a distribution

To demonstrate, let us first generate a vector of random, normally distributed, values.

```{r}
#| echo: true
set.seed(1)
x <- rnorm(n = 100, mean = 2, sd = 5)
```

<br/>

We can then use that vector to demonstrate the functions.

```{r}
#| echo: true
mean(x)
sd(x)
```

<br/>

::: {style="text-align: center;"}
**What are optional arguments for those functions?**

**Why do you think the mean and standard deviation are not exactly those that we would expect?**
:::

## The standard normal distribution

```{r}
slide_mean <- 0; slide_sd <- 1;
```

Standard normal distribution with mean `r slide_mean` and standard deviation `r slide_sd`.

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  pnorm = pnorm(q = quantile, mean = slide_mean, sd = slide_sd)
)
gg1 <- ggplot(x) + geom_point(aes(quantile, pnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("pnorm(q = quantiles, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the cumulative distribution function (c. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = 1, by = 0.01),
  qnorm = qnorm(p = quantile, mean = slide_mean, sd = slide_sd)
)
gg2 <- ggplot(x) + geom_point(aes(quantile, qnorm)) +
  geom_hline(yintercept = slide_mean, color = "blue") +
  geom_label(aes(y = y, label = y), x = 0, data = tibble(y = slide_mean)) +
  geom_hline(yintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(y = y, label = label), x = 0, alpha = 0.5,
    data = tibble(y = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("qnorm(p = probabilities, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the inverse c. d. f.") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  dnorm = dnorm(x = quantile, mean = slide_mean, sd = slide_sd)
)
gg3 <- ggplot(x) + geom_point(aes(quantile, dnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("dnorm(x = quantiles, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the density function (p. f. or p. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  rnorm = rnorm(n = 1E3, mean = slide_mean, sd = slide_sd)
)
gg4 <- ggplot(x, aes(rnorm)) + geom_histogram(bins = 30, color = "black", fill = "grey") +
  geom_rug(alpha = 0.5) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("rnorm(n = 1E3, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "a random variable having the specified distribution") +
  cowplot::theme_cowplot()
```

```{r}
#| fig-align: center
#| fig-width: 12
#| fig-height: 7
plot_grid(gg1, gg2, gg3, gg4, ncol = 2, nrow = 2)
```

## A parameterised normal distribution

```{r}
slide_mean <- 50; slide_sd <- 100;
```

Normal distribution parameterised with mean `r slide_mean` and standard deviation `r slide_sd`.

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  pnorm = pnorm(q = quantile, mean = slide_mean, sd = slide_sd)
)
gg1 <- ggplot(x) + geom_point(aes(quantile, pnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("pnorm(q = quantiles, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the cumulative distribution function (c. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = 1, by = 0.01),
  qnorm = qnorm(p = quantile, mean = slide_mean, sd = slide_sd)
)
gg2 <- ggplot(x) + geom_point(aes(quantile, qnorm)) +
  geom_hline(yintercept = slide_mean, color = "blue") +
  geom_label(aes(y = y, label = y), x = 0, data = tibble(y = slide_mean)) +
  geom_hline(yintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(y = y, label = label), x = 0, alpha = 0.5,
    data = tibble(y = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("qnorm(p = probabilities, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the inverse c. d. f.") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = slide_mean-5*slide_sd, to = slide_mean+5*slide_sd, by = slide_sd/100),
  dnorm = dnorm(x = quantile, mean = slide_mean, sd = slide_sd)
)
gg3 <- ggplot(x) + geom_point(aes(quantile, dnorm)) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("dnorm(x = quantiles, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "the density function (p. f. or p. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  rnorm = rnorm(n = 1E3, mean = slide_mean, sd = slide_sd)
)
gg4 <- ggplot(x, aes(rnorm)) + geom_histogram(bins = 30, color = "black", fill = "grey") +
  geom_rug(alpha = 0.5) +
  geom_vline(xintercept = slide_mean, color = "blue") +
  geom_label(aes(x = x, label = x), y = 0, data = tibble(x = slide_mean)) +
  geom_vline(xintercept = slide_mean+slide_sd*c(-2, 2), color = "blue", linetype = "dashed") +
  geom_label(
    aes(x = x, label = label), y = 0, alpha = 0.5,
    data = tibble(x = slide_mean+slide_sd*c(-2, 2), label = "2*sd")) +
  labs(
    title = sprintf("rnorm(n = 1E3, mean = %s, sd = %s)", slide_mean, slide_sd),
    subtitle = "a random variable having the specified distribution") +
  cowplot::theme_cowplot()
```

```{r}
#| fig-align: center
#| fig-width: 12
#| fig-height: 7
plot_grid(gg1, gg2, gg3, gg4, ncol = 2, nrow = 2)
```

## The binomial distribution

```{r}
#| fig-align: center
## Source: Kevin Rue-Albrecht (Adobe Illustrator)
knitr::include_graphics("img/binomial-coins.svg")
```

:::: {.columns}

::: {.column width="50%"}
- Two mutually exclusive outcomes
  - $P(H) = 1 - P(T)$
- Size of experiment
  - e.g., coins tossed, balls drawn.
- Model number of occurrences of a specific outcome.

:::

::: {.column width="50%"}
```{r}
#| fig-align: center
#| out-height: 300px
## Source: Kevin Rue-Albrecht (Adobe Illustrator)
knitr::include_graphics("img/binomial-balls.svg")
```
:::

::::

## A parameterised binomial distribution

```{r}
slide_size <- 50; slide_prob <- 0.1;
```

Binomial distribution parameterised with size `r slide_size` and probability `r slide_prob`.
This distribution models an experiment where a coin is tossed 50 times, and the probability of observing head is 10%.

```{r}
x <- tibble(
  quantile = seq(from = 0, to = slide_size, by = 1),
  pbinom = pbinom(q = quantile, size = slide_size, prob = slide_prob)
)
gg1 <- ggplot(x) + geom_point(aes(quantile, pbinom)) +
  labs(
    title = sprintf("pbinom(q = quantiles, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "the cumulative distribution function (c. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = 1, by = 0.01),
  qbinom = qbinom(p = quantile, size = slide_size, prob = slide_prob)
)
gg2 <- ggplot(x) + geom_point(aes(quantile, qbinom)) +
  labs(
    title = sprintf("qbinom(p = probabilities, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "the inverse c. d. f.") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  quantile = seq(from = 0, to = slide_size, by = 1),
  dbinom = dbinom(x = quantile, size = slide_size, prob = slide_prob)
)
gg3 <- ggplot(x) + geom_point(aes(quantile, dbinom)) +
  labs(
    title = sprintf("dbinom(x = quantiles, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "the density function (p. f. or p. d. f.)") +
  cowplot::theme_cowplot()
```

```{r}
x <- tibble(
  rbinom = rbinom(n = 1E3, size = slide_size, prob = slide_prob)
)
gg4 <- ggplot(x, aes(rbinom)) + geom_histogram(bins = 30, color = "black", fill = "grey") +
  labs(
    title = sprintf("rbinom(n = 1E3, size = %s, prob = %s)", slide_size, slide_prob),
    subtitle = "a random variable having the specified distribution") +
  cowplot::theme_cowplot()
```

```{r}
#| fig-align: center
#| fig-width: 12
#| fig-height: 7
plot_grid(gg1, gg2, gg3, gg4, ncol = 2, nrow = 2)
```

## Quantiles

Quantiles are the values at a selected set of evenly-spaced locations in the distribution.

```{r}
#| fig-align: center
# Source: Kevin Rue-Albrecht (Adobe Illustrator)
knitr::include_graphics("img/quantiles-quartiles.svg")
```

Quantiles are not always observed values.

```{r}
#| fig-align: center
# Source: Kevin Rue-Albrecht (Adobe Illustrator)
knitr::include_graphics("img/quantiles-ties.svg")
```

## Example - Quantiles of a normal distribution

For instance, the minimum value, the value that separates the lowest 10% values in the distribution, 20%, and so on, until the maximum value.

```{r}
#| fig-align: center
#| fig-height: 6
#| fig-width: 9
slide_mean <- 50; slide_sd <- 100;
x1 <- tibble(
  rnorm = rnorm(n = 10E3, mean = slide_mean, sd = slide_sd)
)
x2 <- tibble(
  quantile = seq(0, 1, length.out = 10),
  value = quantile(x1$rnorm, seq(0, 1, length.out = 10))
)
ggplot() +
  geom_histogram(aes(rnorm), x1, bins = 30, color = "black", fill = "grey") +
  geom_vline(aes(xintercept = value), x2, color = "blue", linetype = "dashed") +
  labs(
    title = sprintf("quantile(x, seq(0, 1, length.out = 10))"),
    subtitle = sprintf("Ten deciles computed on the distribution rnorm(n = 10E3, mean = %s, sd = %s)", slide_mean, slide_sd)) +
  cowplot::theme_cowplot()
```

## Exercise

### Generate and summarise a distribution

- Generate a vector of 1,000 normally distributed values with mean 10 and standard deviation 5.
- Inspect the output of the `summary()` function for that vector.
- Compute the mean and standard deviation for those values.
- Compute the deciles (i.e. 10 evenly spaced quantiles) for those values.
- Visualise the distribution of those values as a histogram.
- Visualise as vertical lines on the histogram: the mean (red solid), median (red dashed), one standard deviation below and above the mean (blue solid), and one median absolute deviation below and above the median (blue dashed).
- Generate a new vector with _a lot_ more values (e.g., one million).
  Draw again a histogram.
  How does the distribution compare with more data points?

## Exercise

### Query distributions and probabilities

For the standard normal distribution ${\mathcal {N}}(\mu=0 ,\sigma ^{2}=1)$:

- Plot the cumulative distribution function in the range $[-5, 5]$ in 0.1 increment.
- Plot the inverse cumulative distribution function for quantiles in 0.01 increment.
- Plot the density function in the range $[-5, 5]$ in 0.1 increment.
- What is the probability of observing a value greater than 2?
- What is the probability of observing a value between -2 and 2?
- What is the probability of observing a value more extreme than -2 or 2?

## Empirical Cumulative Distribution Function

The `ecdf()` function computes an empirical cumulative distribution function.

It produces an object that can be plotted, printed, and used for further computations.

```{r}
#| echo: true
ecdf_iris_sepal_length <- ecdf(iris$Sepal.Length)
ecdf_iris_sepal_length
```

<br/>

The resulting object can be used for downstream analyses, e.g. plotting.

:::: {.columns}

::: {.column width="50%"}
```{r}
#| fig-align: center
#| fig-height: 3
#| fig-width: 5
ggplot(iris, aes(Sepal.Length)) +
  geom_histogram(color = "black", fill = "grey") +
  cowplot::theme_cowplot()
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-align: center
#| fig-height: 3
#| fig-width: 5
plot(ecdf_iris_sepal_length, cex = 0.5)
```
:::

::::

## ecdf - Knots

Knots are the unique values observed in the empirical distribution.
For ease of inspection, the function `knots()` automatically sorts knots in increasing order.

```{r}
#| echo: true
knots(ecdf_iris_sepal_length)
```

<br/>

In other words, the `knots()` function is equivalent to identifying the unique values in the original vector and sorting them in increasing order.

```{r}
#| echo: true
sort(unique(iris$Sepal.Length))
```

## ecdf - Quantiles

The `quantile()` function can be applied to `ecdf` objects.

```{r}
#| echo: true
quantile(ecdf_iris_sepal_length, c(0, 0.25, 0.5, 0.75, 1))
```

<br/>

In this case, the same function `quantile()` can be applied to the original vector of data for the same result.

```{r}
#| echo: true
quantile(iris$Sepal.Length, c(0, 0.25, 0.5, 0.75, 1))
```

::: {style="text-align: center;"}
**Is this choice convenient or confusing to you? Why?**
:::

## <i class="fab fa-r-project"></i> Functions for Statistical Testing

In the `r BiocStyle::CRANpkg("stats")` package, functions that implement statistical tests have a named that ends in `.test`.

```{r}
#| results: asis
cat(paste(sprintf("`%s`", sort(grep("\\.test$", ls("package:stats"), value=TRUE))), collapse = ", "))
```

<br/>

Each of those functions comes with a help page with programmatic usage, statistical advice, and external references to published work.

```{r}
#| echo: true
?pairwise.t.test
help(pairwise.t.test)
```

## The five steps of hypothesis testing

### General principles of hypothesis testing

```{r}
#| fig-align: center
#| out-height: 450px
#| out-width: 700px
## Source: Kevin Rue-Albrecht (Adobe Illustrator)
knitr::include_graphics("img/hypothesis-testing.svg")
```

## Parametric tests and Non-parametric equivalents

```{r}
#| include: false
tibble(
  "Parametric test" = c(
    "Paired t-test",
    "Unpaired t-test",
    "Pearson correlation",
    "One-way Analysis of Variance"),
  "Non-parametric equivalent" = c(
    "Wilcoxon Rank sum test",
    "Mann-Whitney U test",
    "Spearman correlation",
    "Kruskal–Wallis test")
) %>% knitr::kable()
```

When parametric assumptions are not met, non-parametric tests equivalent should be used.

<br/>

|Parametric test              |Non-parametric equivalent |
|:----------------------------|:-------------------------|
|Paired t-test                |Wilcoxon Rank sum test    |
|Unpaired t-test              |Mann-Whitney U test       |
|Pearson correlation          |Spearman correlation      |
|One-way Analysis of Variance |Kruskal–Wallis test       |

<br/>

Non-parametric tests make fewer assumptions, as such:

- they have wider applicability.
- they may be applied in situations where less is known about the data.
- they are more robust.
- ..., however, fewer assumption gives non-parametric tests _less_ power than their parametric equivalent.

::: {.notes}
**Credits:** [https://www.healthknowledge.org.uk/](https://www.healthknowledge.org.uk/public-health-textbook/research-methods/1b-statistical-methods/parametric-nonparametric-tests)
:::

## Parametric t-test

:::: {.columns}

::: {.column width="50%"}
### Two normal distributions

```{r}
#| echo: true
set.seed(1)
x <- rnorm(n = 1000, mean = 0, sd = 1)
y <- rnorm(n = 1000, mean = 1, sd = 1)
```

```{r}
#| out-height: 400px
test_data <- bind_rows(
  tibble(group = "x", value = x),
  tibble(group = "y", value = y)
)
ggplot(test_data, aes(value)) +
  geom_histogram(fill = "grey", color = "black") +
  facet_wrap(~group, ncol = 1) +
  cowplot::theme_cowplot()
```
:::

::: {.column width="50%"}
### Unpaired t-test

```{r}
#| echo: true
t.test(value ~ group, test_data)
```

<br/>

Compare with

```{r}
#| echo: true
#| eval: false
t.test(x, y)
t.test(y, x)
```
:::

::::

## Non-parametric wilcoxon test

:::: {.columns}

::: {.column width="50%"}
### Two uniform distributions

```{r}
#| echo: true
set.seed(1)
x <- runif(n = 1000, min = 1, max = 11)
y <- runif(n = 1000, min = 3, max = 13)
```

```{r}
#| fig-align: center
#| out-height: 400px
test_data <- bind_rows(
  tibble(group = "x", value = x),
  tibble(group = "y", value = y)
)
gg <- ggplot(test_data, aes(value)) +
  facet_wrap(~group, ncol = 1) +
  geom_histogram(fill = "grey", color = "black") +
  cowplot::theme_cowplot()
gg
```
:::

::: {.column width="50%"}
### Mann-Whitney U test

```{r}
#| echo: true
wilcox.test(value ~ group, test_data)
```

<br/>

### Directed hypothesis

```{r}
#| echo: true
wilcox.test(value ~ group, test_data, alternative = "less")
```
:::

::::

## Paired test

For each sample, the two measurements are related to one another; e.g. patients measured before and after a treatment.

:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: true
set.seed(1)
n_sample <- 10
x <- runif(n = n_sample, min = 10, max = 20)
y <- x + 2 + rnorm(n = n_sample, mean = 0, sd = 1)
```

```{r}
test_data <- tibble(
  sample = paste("sample", seq_len(n_sample)),
  x = x,
  y = y
) %>% pivot_longer(cols = c(x, y), names_to = "variable")
```

```{r}
#| out-height: 300px
ggplot(test_data, aes(variable, value)) +
  geom_line(aes(group = sample), size = 0.1) +
  geom_point() +
  cowplot::theme_cowplot()
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
t.test(x, y, paired = TRUE)
```
:::

::::

**Note:**
What is actually tested is whether the mean of the differences between the paired $x$ and $y$ measurements is different from 0.

## Analysis of Variance (ANOVA)

Is the variance _between_ groups larger than the variance _within_ groups?

```{r}
#| fig-align: center
#| out-height: 450px
#| out-width: 700px
# Source: https://turnthewheelsandbox.wordpress.com/2015/08/11/lesson-12-intro-to-one-way-anova/
knitr::include_graphics("img/anova.png")
```

`r Citet(bib, "liguori2018data")`

## Analysis of Variance (ANOVA)

:::: {.columns}

::: {.column width="50%"}
```{r}
set.seed(1)
n_sample <- 1000
x1 <- rnorm(n = n_sample, mean = 10, sd = 2)
x2 <- x1 + 5 + rnorm(n = n_sample, mean = 0, sd = 1)
x3 <- x2 + 0 + rnorm(n = n_sample, mean = 0, sd = 0.5)
test_data <- bind_rows(
  tibble(group = "x1", value = x1),
  tibble(group = "x2", value = x2),
  tibble(group = "x3", value = x3)
)
```

```{r}
#| fig-height: 10
test_data <- bind_rows(
  tibble(group = "x1", value = x1),
  tibble(group = "x2", value = x2),
  tibble(group = "x3", value = x3)
)
gg <- ggplot(test_data, aes(value)) +
  facet_wrap(~group, ncol = 1) +
  geom_histogram(fill = "grey", color = "black") +
  cowplot::theme_cowplot()
gg
```
:::

::: {.column width="50%"}
<br/>

```{r}
#| echo: true
out <- aov(value ~ group, test_data)
out
```

<br/>

```{r}
#| echo: true
summary(out)
```
:::

::::

## Linear models

Describe a continuous response variable as a function of one or more predictor variables.

:::: {.columns}

::: {.column width="50%"}
```{r}
set.seed(1)
test_data <- tibble(
  x = rnorm(n = 50, mean = 0, sd = 1),
  y = 10 + 2.5 * x + rnorm(n = 50, mean = 0, sd = 0.5))
```

```{r}
#| fig-align: center
#| fig-width: 6
#| fig-height: 4
#| out-height: 200px
#| out-width: 300px
ggplot(test_data, aes(x = x, y = y)) +
  geom_point() +
  stat_smooth(method = "glm", se = FALSE) +
  cowplot::theme_cowplot()
```

- What is the slope?
- What is the intercept?

Null hypotheses:

- The slope is equal to 0.
- The intercept is equal to 0.
:::

::: {.column width="50%"}
<br/>

```{r}
#| echo: true
lm(y ~ x, test_data)
```
:::

::::

## Linear models - summary

<br/>

```{r}
#| echo: true
lm(y ~ x, test_data) %>% summary()
```

## Fisher's Exact Test

- Test of independence between two categorical variables
- Alternative to the Chi-square test when the sample is not large enough.
  - Rule of thumb: when any of the _expected_ values in the contingency table is less than 5
  - e.g., Gene set over-representation analysis (ORA)

:::: {.columns}

::: {.column width="50%"}
```{r}
## Source: Kevin Rue-Albrecht (Adobe Illustrator)
knitr::include_graphics("img/fisher-test-table.svg")
```
:::

::: {.column width="50%"}
```{r}
## Source: Kevin Rue-Albrecht (Adobe Illustrator)
knitr::include_graphics("img/fisher-test-venn.svg")
```
:::

::::

Further reading:
[Towards data science](https://towardsdatascience.com/fishers-exact-test-in-r-independence-test-for-a-small-sample-56965db48e87)

## Fisher's Exact Test

<br/>

|               |    DE | Not DE |  Total |
|:--------------|------:|-------:|-------:|
|In pathway     |   $a$ |    $b$ |  $a+b$ |
|Not in pathway |   $c$ |    $d$ |  $c+d$ |
|Total          | $a+c$ |  $b+d$ | $a+b+c+d$ $(=n)$ |

<br/>

::: {style="text-align: center;"}
**What is the probability of observing a given distribution?**
:::

<br/>

$$p = \frac{{a+b \choose a} {c+d \choose c}} {n \choose a+c} = \frac{{a+b \choose b}{c+d \choose d}}{n \choose b+d}= \frac{(a+b)!~(c+d)!~(a+c)!~(b+d)!}{a!~b!~c!~d!~n!}$$

::: {.notes}
Source: <https://en.wikipedia.org/wiki/Fisher%27s_exact_test>
:::

## Fisher's Exact Test

<br/>

```{r}
x_table <- matrix(data = c(12, 4, 3, 23),
  nrow = 2,
  dimnames = list(
    c("DE","Not DE"),
    c("in_pathway", "not_pathway")
  ))
knitr::kable(x_table)
```

<br/>

```{r}
#| echo: true
fisher.test(x_table)
```

## Beware of interpreting inadequate tests!

<br/>

:::: {.columns}

::: {.column width="50%"}
### Two uniform distributions

```{r}
#| echo: true
set.seed(1)
n_size <- 10E3
x <- runif(n = n_size, min = 1, max = 11)
y <- runif(n = n_size, min = 3, max = 13)
```

```{r}
test_data <- bind_rows(
  tibble(group = "x", value = x),
  tibble(group = "y", value = y)
)
gg <- ggplot(test_data, aes(value)) +
  facet_wrap(~group, ncol = 1) +
  geom_histogram(fill = "grey", color = "black") +
  cowplot::theme_cowplot()
gg
```
:::

::: {.column width="50%"}
### Parametric (unpaired) t-test

```{r}
#| echo: true
t.test(value ~ group, test_data)
```
:::

::::

## Choosing a test

```{r}
#| out-width: 600px
#| out-height: 400px
#| fig-align: center
# Source: https://python.plainenglish.io/statistical-tests-with-python-880251e9b572
knitr::include_graphics("img/choose-statistical-test.png")
```

`r Citet(bib, "liguori2018data")`

See also <https://stats.idre.ucla.edu/other/mult-pkg/whatstat/>

## Knowledge assumptions - Central tendency

Tests make assumptions that must be met to for the results to be interpreted properly and with validity.

For instance, Student's t-Test expects values to be located around a central or typical value.

:::: {.columns}

::: {.column width="50%"}
```{r}
x_mean <- 0
x_sd <- 20
data_table <- tibble(x = as.integer(rnorm(n = 10E3, mean = x_mean, sd = x_sd)))
summary_table <- bind_rows(
  tibble(Value = "mean", value = mean(data_table$x)),
  tibble(Value = "median", value = median(data_table$x)),
  tibble(Value = "mode", value = as.integer(names(which.max(table(data_table$x)))))
)
data_table %>% 
  ggplot() +
  geom_histogram(aes(x = x), color = "black", fill = "grey") +
  geom_vline(aes(xintercept = value, color = Value), summary_table, size = 2, alpha = 0.3) +
  cowplot::theme_cowplot()
```
:::

::: {.column width="50%"}
Measures of central tendency include:

- the arithmetic mean
- the median
- ...
:::

::::

## Knowledge assumptions - Normality

In addition, Student's t-Test also expects values to be normally distributed.

:::: {.columns}

::: {.column width="50%"}
### Normal distribution

```{r}
#| echo: true
x <- rnorm(n = 5000, mean = 0, sd = 1)
```

```{r}
ggplot(tibble(x=x)) +
  geom_histogram(aes(x), fill = "grey", color = "black", bins = 20) +
  theme_cowplot()
```

```{r}
#| echo: true
shapiro.test(x)
```
:::

::: {.column width="50%"}
### Log-normal distribution

```{r}
#| echo: true
x <- 2^rnorm(n = 5000, mean = 0, sd = 1)
```

```{r}
#| fig-align: center
ggplot(tibble(x=x)) +
  geom_histogram(aes(x), fill = "grey", color = "black", bins = 20) +
  theme_cowplot()
```

```{r}
#| echo: true
shapiro.test(x)
```
:::

::::

## Knowledge assumptions - Normality

The Quantile-Quantile Plots (QQ plot) contrasts the quantiles of the observed distribution to those of a theoretical distribution.

:::: {.columns}

::: {.column width="50%"}
### Normal distribution

```{r}
#| echo: true
#| fig-align: center
x <- rnorm(n = 5000, mean = 5, sd = 3)
qqnorm(x)
```
:::

::: {.column width="50%"}
### Log-normal distribution

```{r}
#| echo: true
#| fig-align: center
x <- 2^rnorm(n = 5000, mean = 0, sd = 1)
qqnorm(x)
```
:::

::::

## Multiple-testing correction

### Hypothesis

"Jelly beans cause acne."

### Results

:::: {.columns}

::: {.column width="50%"}
- No link between jelly beans and acne.
:::

::: {.column width="50%"}
- No link between _brown_ jelly beans and acne.
- No link between _pink_ jelly beans and acne.
- ...
- Link between _green_ jelly beans and acne.
:::

::::

### Newspaper

> Green jelly beans linked to acne!
  95% confidence!
  Only 5% chance of coincidence!

[https://xkcd.com/882/](https://imgs.xkcd.com/comics/significant.png)

## Multiple-testing correction

:::: {.columns}

::: {.column width="50%"}
Distribution of $p$-values in an RNA-seq differential expression experiment

- True positive
- True negative
- False positive (type I error)
- False negative (type I error)

```{r}
#| fig-align: center
#| out-width: 300px
#| out-height: 300px
## Source: https://www.huber.embl.de/msmb/Chap-Testing.html
knitr::include_graphics("img/huber-fdr.png")
```
:::

::: {.column width="50%"}

::: {style="text-align: center;"}
Bonferroni correction
:::

```{r}
#| fig-align: center
#| out-width: 200px
#| out-height: 200px
## Source: https://www.huber.embl.de/msmb/Chap-Testing.html
knitr::include_graphics("img/huber-bonferroni.png")
```

::: {style="text-align: center;"}
Benjamini-Hochberg procedure
:::

```{r}
#| fig-align: center
#| out-width: 200px
#| out-height: 200px
## Source: https://www.huber.embl.de/msmb/Chap-Testing.html
knitr::include_graphics("img/huber-bh.png")
```
:::

::::

## Multiple-testing correction

```{r}
set.seed(1)
n_tests <- 1000
compute_p_value <- function(dummy) {
  x <- rnorm(n = 100, mean = 0, sd = 1)
  y <- rnorm(n = 100, mean = 0, sd = 1)
  out <- t.test(x, y)
  out$p.value
}
result_table <- tibble(
  pvalue = vapply(X = seq_len(n_tests), FUN = compute_p_value, FUN.VALUE = numeric(1)),
  BH = p.adjust(p = pvalue, method = "BH"),
  bonferroni = p.adjust(p = pvalue, method = "bonferroni")
)
```

:::: {.columns}

::: {.column width="50%"}
Let us carry `r n_tests` tests between two normal distributions of mean 0 and standard deviation 1.

```{r}
#| fig-align: center
#| fig-height: 3
data_table <- tibble(
  x = rnorm(n = 100, mean = 0, sd = 1),
  y = rnorm(n = 100, mean = 0, sd = 1)
) %>% pivot_longer(cols = c(x, y))
ggplot(data_table) +
  geom_boxplot(aes(name, value)) +
  geom_jitter(aes(name, value), width = 0.1)
```

```{r}
#| fig-align: center
#| fig-height: 3
ggplot(result_table) +
  geom_histogram(aes(pvalue), fill = "grey", color = "black", breaks = seq(0, 1, 0.05)) +
  scale_x_continuous(limits = c(0, 1)) +
  labs(title = "Raw p-value")
```

```{r}
#| results: asis
cat(sprintf("There are %i out of %i raw p-values smaller than 0.05", sum(result_table$pvalue < 0.05), n_tests))
```
:::

::: {.column width="50%"}
```{r}
#| fig-align: center
#| fig-height: 3
ggplot(result_table) +
  geom_histogram(aes(BH), fill = "grey", color = "black", bins = 20) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(title = "BH correction")
```

```{r}
#| results: asis
cat(sprintf("There are %i BH-corrected p-values smaller than 0.05", sum(result_table$BH < 0.05)))
```

```{r}
#| fig-align: center
#| fig-height: 3
ggplot(result_table) +
  geom_histogram(aes(bonferroni), fill = "grey", color = "black", bins = 20) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(title = "bonferroni correction")
```

```{r}
#| results: asis
cat(sprintf("There are %i bonferonni corrected p-values smaller than 0.05", sum(result_table$bonferroni < 0.05)))
```
:::

::::

## Multiple-testing correction in <i class="fab fa-r-project"></i>

The `p.adjust()` function can run a number of methods for multiple testing correction.

The `p.adjust.methods` object list the available methods.

The function takes a vector of p-values and the name of a method.

<br/>

```{r}
#| echo: true
#| eval: false
p.adjust(p_values, method = "bonferroni")
```

## Exercise

:::: {.columns}

::: {.column width="50%"}
### Statistical tests

#### Wilcoxon test

- Run a Wilcoxon test to compare the vectors `1:10` and `5:14`.

::: {style="text-align: center;"}
**What is the p-value?**
:::

#### T-test

- Import the data in the file `gene_exprs.csv`.

- Run a t-test to compare the gene expression values between the two groups.

::: {style="text-align: center;"}
**What is the p-value?**
:::
:::

::: {.column width="50%"}
```{r}
#| fig-align: center
#| fig-height: 4
#| fig-width: 5
#| out-height: 250px 
#| out-width: 300px
df <- data.frame(
    value = c(1:10, 5:14),
    group = rep(c("x", "y"), each = 10)
)
ggplot(df, aes(group, value)) +
    geom_point() +
    coord_cartesian(ylim = c(0, 15))
```

```{r}
#| fig-align: center
#| fig-height: 4
#| fig-width: 5
#| out-height: 250px 
#| out-width: 300px
df <- read.csv("data/gene_exprs.csv")
ggplot(df, aes(group, gene_exprs)) +
    geom_point()
```
:::

::::

## Exercise

### Testing & Multiple testing correction

Given an Excel file that contains a matrix of log-normalised counts (`logcounts`) and experimental metadata (`cell_info`),
test each gene (i.e., row) in the matrix for differential expression between the two experimental groups.
Start by importing the `logcounts` table and converting it to a matrix.

#### Approach

1. Write the code to test a single gene and access the p-value.
2. Write a function that generalises the code to test any one gene and return the p-value.
3. Use the function `vapply` to test every row in the matrix and collect a vector of p-values.

#### Bonus points

- Visualise a histogram of the p-values.

- Correct p-values for multiple testing.
  How many genes remain before and after multiple testing?

- Use `gene_info` to get the gene name for the gene identifier with the smallest p-value.

## Exercise

### Illustration

```{r}
#| fig-align: center
knitr::include_graphics("img/apply-gene-expression.svg")
```

## Exercise

### Over-representation analysis (ORA)

Given the list of genes (Ensembl gene identifiers) that your identified as differentially expressed in the previous exercise,
and a list of gene sets (`go_db`),
test each gene set for over-representation of differentially expressed genes.
Start by importing the Gene Ontology table and converting it to a list.

#### Approach

1. Write the code to test a single gene set and access the p-value.
2. Write a function that generalises the code to test any one gene set and return the p-value.
3. Use the function `vapply` to test every gene set in the list and collect a vector of p-values.

#### Bonus points

- Visualise a bar plot of the p-values.
- Correct p-values for multiple testing.
  How many gene sets remain before and after multiple testing?
- Use `go_info` to annotate each GO gene set with its corrected p-value,
  and arrange the table by increasing p-value.

## Further reading

- [UCLouvain Bioinformatics Summer School 2019](https://uclouvain-cbio.github.io/BSS2019/)
  - [Introduction to Statistics and Machine Learning](https://github.com/ococrook/2019-BSS/raw/master/Intro2statsml.pdf) by Oliver M. Crook
  - [Practical: stats/ML](https://htmlpreview.github.io/?https://github.com/ococrook/2019-BSS/blob/master/practical/Intro2statmlPractical.html)
- [CSAMA](https://github.com/Bioconductor/CSAMA/tree/2019/lecture/1-monday) by the European Molecular Biology Laboratory (EMBL).
- [Statistic with R and dplyr and ggplot](https://www.youtube.com/watch?v=ANMuuq502rE) by Greg Martin
- [Susan Holmes](http://statweb.stanford.edu/~susan/) - [Introduction to Statistics for Biology and Biostatistics](http://statweb.stanford.edu/~susan/courses/s141/)
- Susan Holmes & Wolfgang Huber - [Modern Statistics for Modern Biology: Testing](https://www.huber.embl.de/msmb/Chap-Testing.html)
- [Bioconductor Case Studies](https://www.bioconductor.org/help/publications/books/bioconductor-case-studies/)
- [Introduction to Econometrics with R](https://www.econometrics-with-r.org/6-6-exercises-4.html)

## References

```{r}
#| results: asis
PrintBibliography(bib)
```
